{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c0af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 22, 1000]             512\n",
      "       BatchNorm2d-2          [-1, 8, 22, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]             352\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "           Linear-17               [-1, 15, 16]             272\n",
      "           Linear-18               [-1, 15, 16]             272\n",
      "           Linear-19               [-1, 15, 16]             272\n",
      "          Dropout-20            [-1, 2, 15, 15]               0\n",
      "           Linear-21               [-1, 15, 16]             272\n",
      "MultiHeadAttention-22               [-1, 15, 16]               0\n",
      "          Dropout-23               [-1, 15, 16]               0\n",
      "        LayerNorm-24               [-1, 15, 16]              32\n",
      "      ResidualAdd-25               [-1, 15, 16]               0\n",
      "           Linear-26               [-1, 15, 64]           1,088\n",
      "             GELU-27               [-1, 15, 64]               0\n",
      "          Dropout-28               [-1, 15, 64]               0\n",
      "           Linear-29               [-1, 15, 16]           1,040\n",
      "          Dropout-30               [-1, 15, 16]               0\n",
      "        LayerNorm-31               [-1, 15, 16]              32\n",
      "      ResidualAdd-32               [-1, 15, 16]               0\n",
      "           Linear-33               [-1, 15, 16]             272\n",
      "           Linear-34               [-1, 15, 16]             272\n",
      "           Linear-35               [-1, 15, 16]             272\n",
      "          Dropout-36            [-1, 2, 15, 15]               0\n",
      "           Linear-37               [-1, 15, 16]             272\n",
      "MultiHeadAttention-38               [-1, 15, 16]               0\n",
      "          Dropout-39               [-1, 15, 16]               0\n",
      "        LayerNorm-40               [-1, 15, 16]              32\n",
      "      ResidualAdd-41               [-1, 15, 16]               0\n",
      "           Linear-42               [-1, 15, 64]           1,088\n",
      "             GELU-43               [-1, 15, 64]               0\n",
      "          Dropout-44               [-1, 15, 64]               0\n",
      "           Linear-45               [-1, 15, 16]           1,040\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "      ResidualAdd-48               [-1, 15, 16]               0\n",
      "           Linear-49               [-1, 15, 16]             272\n",
      "           Linear-50               [-1, 15, 16]             272\n",
      "           Linear-51               [-1, 15, 16]             272\n",
      "          Dropout-52            [-1, 2, 15, 15]               0\n",
      "           Linear-53               [-1, 15, 16]             272\n",
      "MultiHeadAttention-54               [-1, 15, 16]               0\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "        LayerNorm-56               [-1, 15, 16]              32\n",
      "      ResidualAdd-57               [-1, 15, 16]               0\n",
      "           Linear-58               [-1, 15, 64]           1,088\n",
      "             GELU-59               [-1, 15, 64]               0\n",
      "          Dropout-60               [-1, 15, 64]               0\n",
      "           Linear-61               [-1, 15, 16]           1,040\n",
      "          Dropout-62               [-1, 15, 16]               0\n",
      "        LayerNorm-63               [-1, 15, 16]              32\n",
      "      ResidualAdd-64               [-1, 15, 16]               0\n",
      "           Linear-65               [-1, 15, 16]             272\n",
      "           Linear-66               [-1, 15, 16]             272\n",
      "           Linear-67               [-1, 15, 16]             272\n",
      "          Dropout-68            [-1, 2, 15, 15]               0\n",
      "           Linear-69               [-1, 15, 16]             272\n",
      "MultiHeadAttention-70               [-1, 15, 16]               0\n",
      "          Dropout-71               [-1, 15, 16]               0\n",
      "        LayerNorm-72               [-1, 15, 16]              32\n",
      "      ResidualAdd-73               [-1, 15, 16]               0\n",
      "           Linear-74               [-1, 15, 64]           1,088\n",
      "             GELU-75               [-1, 15, 64]               0\n",
      "          Dropout-76               [-1, 15, 64]               0\n",
      "           Linear-77               [-1, 15, 16]           1,040\n",
      "          Dropout-78               [-1, 15, 16]               0\n",
      "        LayerNorm-79               [-1, 15, 16]              32\n",
      "      ResidualAdd-80               [-1, 15, 16]               0\n",
      "           Linear-81               [-1, 15, 16]             272\n",
      "           Linear-82               [-1, 15, 16]             272\n",
      "           Linear-83               [-1, 15, 16]             272\n",
      "          Dropout-84            [-1, 2, 15, 15]               0\n",
      "           Linear-85               [-1, 15, 16]             272\n",
      "MultiHeadAttention-86               [-1, 15, 16]               0\n",
      "          Dropout-87               [-1, 15, 16]               0\n",
      "        LayerNorm-88               [-1, 15, 16]              32\n",
      "      ResidualAdd-89               [-1, 15, 16]               0\n",
      "           Linear-90               [-1, 15, 64]           1,088\n",
      "             GELU-91               [-1, 15, 64]               0\n",
      "          Dropout-92               [-1, 15, 64]               0\n",
      "           Linear-93               [-1, 15, 16]           1,040\n",
      "          Dropout-94               [-1, 15, 16]               0\n",
      "        LayerNorm-95               [-1, 15, 16]              32\n",
      "      ResidualAdd-96               [-1, 15, 16]               0\n",
      "           Linear-97               [-1, 15, 16]             272\n",
      "           Linear-98               [-1, 15, 16]             272\n",
      "           Linear-99               [-1, 15, 16]             272\n",
      "         Dropout-100            [-1, 2, 15, 15]               0\n",
      "          Linear-101               [-1, 15, 16]             272\n",
      "MultiHeadAttention-102               [-1, 15, 16]               0\n",
      "         Dropout-103               [-1, 15, 16]               0\n",
      "       LayerNorm-104               [-1, 15, 16]              32\n",
      "     ResidualAdd-105               [-1, 15, 16]               0\n",
      "          Linear-106               [-1, 15, 64]           1,088\n",
      "            GELU-107               [-1, 15, 64]               0\n",
      "         Dropout-108               [-1, 15, 64]               0\n",
      "          Linear-109               [-1, 15, 16]           1,040\n",
      "         Dropout-110               [-1, 15, 16]               0\n",
      "       LayerNorm-111               [-1, 15, 16]              32\n",
      "     ResidualAdd-112               [-1, 15, 16]               0\n",
      "         Flatten-113                  [-1, 240]               0\n",
      "         Dropout-114                  [-1, 240]               0\n",
      "          Linear-115                    [-1, 4]             964\n",
      "================================================================\n",
      "Total params: 25,684\n",
      "Trainable params: 25,684\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.43\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 3.61\n",
      "----------------------------------------------------------------\n",
      "Thu Oct 17 04:49:43 2024\n",
      "seed is 966\n",
      "Subject 1\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "1_0 train_acc: 0.3109 train_loss: 2.535962\tval_acc: 0.313725 val_loss: 1.3630854\n",
      "1_1 train_acc: 0.2360 train_loss: 2.346679\tval_acc: 0.357843 val_loss: 1.3596708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_2 train_acc: 0.3146 train_loss: 1.977199\tval_acc: 0.436275 val_loss: 1.2736508\n",
      "1_3 train_acc: 0.3333 train_loss: 1.755401\tval_acc: 0.465686 val_loss: 1.2389588\n",
      "1_4 train_acc: 0.3109 train_loss: 1.899612\tval_acc: 0.504902 val_loss: 1.2232871\n",
      "1_5 train_acc: 0.3820 train_loss: 1.592768\tval_acc: 0.563725 val_loss: 1.1546565\n",
      "1_6 train_acc: 0.4045 train_loss: 1.511896\tval_acc: 0.612745 val_loss: 1.0918190\n",
      "1_7 train_acc: 0.4195 train_loss: 1.373556\tval_acc: 0.671569 val_loss: 0.9923918\n",
      "1_8 train_acc: 0.4045 train_loss: 1.432190\tval_acc: 0.661765 val_loss: 0.9919081\n",
      "1_9 train_acc: 0.4607 train_loss: 1.275380\tval_acc: 0.681373 val_loss: 0.9286916\n",
      "1_10 train_acc: 0.4120 train_loss: 1.381421\tval_acc: 0.715686 val_loss: 0.8334206\n",
      "1_11 train_acc: 0.5169 train_loss: 1.131351\tval_acc: 0.750000 val_loss: 0.7718888\n",
      "1_12 train_acc: 0.5393 train_loss: 1.087529\tval_acc: 0.759804 val_loss: 0.7452695\n",
      "1_13 train_acc: 0.4944 train_loss: 1.200974\tval_acc: 0.769608 val_loss: 0.6812358\n",
      "1_14 train_acc: 0.5805 train_loss: 1.095566\tval_acc: 0.774510 val_loss: 0.6608857\n",
      "1_15 train_acc: 0.6067 train_loss: 0.902259\tval_acc: 0.754902 val_loss: 0.6465942\n",
      "1_16 train_acc: 0.6517 train_loss: 0.811048\tval_acc: 0.759804 val_loss: 0.6400529\n",
      "1_17 train_acc: 0.6067 train_loss: 0.895185\tval_acc: 0.769608 val_loss: 0.5819804\n",
      "1_18 train_acc: 0.6404 train_loss: 0.900450\tval_acc: 0.769608 val_loss: 0.5599763\n",
      "1_19 train_acc: 0.6742 train_loss: 0.796824\tval_acc: 0.784314 val_loss: 0.5477774\n",
      "1_26 train_acc: 0.7228 train_loss: 0.656120\tval_acc: 0.799020 val_loss: 0.5009167\n",
      "1_31 train_acc: 0.7191 train_loss: 0.633014\tval_acc: 0.794118 val_loss: 0.4963989\n",
      "1_39 train_acc: 0.8090 train_loss: 0.515267\tval_acc: 0.799020 val_loss: 0.4708762\n",
      "1_40 train_acc: 0.7528 train_loss: 0.565078\tval_acc: 0.779412 val_loss: 0.4697056\n",
      "1_41 train_acc: 0.7378 train_loss: 0.553579\tval_acc: 0.803922 val_loss: 0.4427410\n",
      "1_48 train_acc: 0.8315 train_loss: 0.465127\tval_acc: 0.818627 val_loss: 0.4222688\n",
      "1_50 train_acc: 0.7790 train_loss: 0.491981\tval_acc: 0.823529 val_loss: 0.3810555\n",
      "1_61 train_acc: 0.8277 train_loss: 0.475652\tval_acc: 0.833333 val_loss: 0.3675122\n",
      "1_62 train_acc: 0.8052 train_loss: 0.449461\tval_acc: 0.848039 val_loss: 0.3373981\n",
      "1_64 train_acc: 0.8127 train_loss: 0.450668\tval_acc: 0.862745 val_loss: 0.3266028\n",
      "1_71 train_acc: 0.8090 train_loss: 0.477662\tval_acc: 0.867647 val_loss: 0.3204530\n",
      "1_75 train_acc: 0.8165 train_loss: 0.425116\tval_acc: 0.882353 val_loss: 0.3024269\n",
      "1_76 train_acc: 0.8352 train_loss: 0.384401\tval_acc: 0.872549 val_loss: 0.2670719\n",
      "1_80 train_acc: 0.8352 train_loss: 0.390166\tval_acc: 0.901961 val_loss: 0.2624680\n",
      "1_84 train_acc: 0.8315 train_loss: 0.428280\tval_acc: 0.887255 val_loss: 0.2403191\n",
      "1_90 train_acc: 0.8352 train_loss: 0.393040\tval_acc: 0.911765 val_loss: 0.2154052\n",
      "1_94 train_acc: 0.8989 train_loss: 0.314005\tval_acc: 0.926471 val_loss: 0.1901377\n",
      "1_109 train_acc: 0.8689 train_loss: 0.317657\tval_acc: 0.916667 val_loss: 0.1843398\n",
      "1_117 train_acc: 0.8764 train_loss: 0.270748\tval_acc: 0.921569 val_loss: 0.1764583\n",
      "1_122 train_acc: 0.8614 train_loss: 0.379602\tval_acc: 0.946078 val_loss: 0.1593895\n",
      "1_134 train_acc: 0.8914 train_loss: 0.289787\tval_acc: 0.950980 val_loss: 0.1375493\n",
      "1_160 train_acc: 0.9064 train_loss: 0.253031\tval_acc: 0.931373 val_loss: 0.1372635\n",
      "1_166 train_acc: 0.8951 train_loss: 0.255369\tval_acc: 0.955882 val_loss: 0.1355832\n",
      "1_175 train_acc: 0.9026 train_loss: 0.251107\tval_acc: 0.955882 val_loss: 0.1096608\n",
      "1_197 train_acc: 0.9251 train_loss: 0.198139\tval_acc: 0.955882 val_loss: 0.1074946\n",
      "1_205 train_acc: 0.9176 train_loss: 0.221368\tval_acc: 0.975490 val_loss: 0.1064415\n",
      "1_213 train_acc: 0.9139 train_loss: 0.207908\tval_acc: 0.970588 val_loss: 0.1013562\n",
      "1_218 train_acc: 0.9026 train_loss: 0.258418\tval_acc: 0.960784 val_loss: 0.0944402\n",
      "1_224 train_acc: 0.9026 train_loss: 0.250580\tval_acc: 0.970588 val_loss: 0.0937613\n",
      "1_230 train_acc: 0.9213 train_loss: 0.203040\tval_acc: 0.975490 val_loss: 0.0894531\n",
      "1_286 train_acc: 0.9213 train_loss: 0.204913\tval_acc: 0.955882 val_loss: 0.0785341\n",
      "1_314 train_acc: 0.9476 train_loss: 0.183360\tval_acc: 0.975490 val_loss: 0.0749784\n",
      "1_320 train_acc: 0.9363 train_loss: 0.184366\tval_acc: 0.990196 val_loss: 0.0612284\n",
      "1_351 train_acc: 0.9588 train_loss: 0.118487\tval_acc: 0.985294 val_loss: 0.0549327\n",
      "1_407 train_acc: 0.9513 train_loss: 0.140409\tval_acc: 0.985294 val_loss: 0.0504748\n",
      "1_423 train_acc: 0.9513 train_loss: 0.154974\tval_acc: 0.995098 val_loss: 0.0450862\n",
      "1_437 train_acc: 0.9363 train_loss: 0.175982\tval_acc: 0.990196 val_loss: 0.0431807\n",
      "1_458 train_acc: 0.9288 train_loss: 0.192286\tval_acc: 0.980392 val_loss: 0.0427299\n",
      "1_466 train_acc: 0.9288 train_loss: 0.231087\tval_acc: 0.985294 val_loss: 0.0401519\n",
      "1_512 train_acc: 0.9326 train_loss: 0.141133\tval_acc: 0.995098 val_loss: 0.0376386\n",
      "1_524 train_acc: 0.9625 train_loss: 0.099639\tval_acc: 0.990196 val_loss: 0.0376042\n",
      "1_540 train_acc: 0.9663 train_loss: 0.129360\tval_acc: 0.995098 val_loss: 0.0345074\n",
      "1_585 train_acc: 0.9588 train_loss: 0.100766\tval_acc: 0.995098 val_loss: 0.0328312\n",
      "1_604 train_acc: 0.9700 train_loss: 0.111133\tval_acc: 0.990196 val_loss: 0.0316437\n",
      "1_645 train_acc: 0.9476 train_loss: 0.152248\tval_acc: 1.000000 val_loss: 0.0306740\n",
      "1_657 train_acc: 0.9476 train_loss: 0.135476\tval_acc: 0.995098 val_loss: 0.0265677\n",
      "1_665 train_acc: 0.9326 train_loss: 0.155864\tval_acc: 0.990196 val_loss: 0.0252707\n",
      "1_724 train_acc: 0.9476 train_loss: 0.116931\tval_acc: 1.000000 val_loss: 0.0240768\n",
      "1_730 train_acc: 0.9326 train_loss: 0.134731\tval_acc: 0.995098 val_loss: 0.0226144\n",
      "1_744 train_acc: 0.9588 train_loss: 0.098092\tval_acc: 1.000000 val_loss: 0.0198808\n",
      "1_751 train_acc: 0.9738 train_loss: 0.080489\tval_acc: 1.000000 val_loss: 0.0163279\n",
      "1_835 train_acc: 0.9551 train_loss: 0.149600\tval_acc: 1.000000 val_loss: 0.0162192\n",
      "1_854 train_acc: 0.9363 train_loss: 0.138922\tval_acc: 1.000000 val_loss: 0.0154038\n",
      "1_874 train_acc: 0.9513 train_loss: 0.142055\tval_acc: 1.000000 val_loss: 0.0116995\n",
      "1_898 train_acc: 0.9588 train_loss: 0.099088\tval_acc: 1.000000 val_loss: 0.0105873\n",
      "1_979 train_acc: 0.9813 train_loss: 0.061185\tval_acc: 1.000000 val_loss: 0.0095641\n",
      "epoch:  979 \tThe test accuracy is: 0.9097222222222222\n",
      " THE BEST ACCURACY IS 0.9097222222222222\tkappa is 0.8796296296296297\n",
      "subject 1 duration: 0:20:03.816651\n",
      "seed is 100\n",
      "Subject 2\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "2_0 train_acc: 0.2809 train_loss: 2.586660\tval_acc: 0.254902 val_loss: 1.6863086\n",
      "2_1 train_acc: 0.2809 train_loss: 2.205049\tval_acc: 0.245098 val_loss: 1.5238651\n",
      "2_2 train_acc: 0.3371 train_loss: 2.025725\tval_acc: 0.308824 val_loss: 1.3669782\n",
      "2_3 train_acc: 0.2996 train_loss: 1.963963\tval_acc: 0.411765 val_loss: 1.2946539\n",
      "2_4 train_acc: 0.3109 train_loss: 1.772199\tval_acc: 0.460784 val_loss: 1.2416013\n",
      "2_5 train_acc: 0.3221 train_loss: 1.642000\tval_acc: 0.475490 val_loss: 1.2122343\n",
      "2_6 train_acc: 0.3708 train_loss: 1.599087\tval_acc: 0.529412 val_loss: 1.1711974\n",
      "2_8 train_acc: 0.3633 train_loss: 1.614436\tval_acc: 0.544118 val_loss: 1.1539056\n",
      "2_9 train_acc: 0.3783 train_loss: 1.414747\tval_acc: 0.642157 val_loss: 1.0902512\n",
      "2_10 train_acc: 0.3783 train_loss: 1.447017\tval_acc: 0.612745 val_loss: 1.0801158\n",
      "2_11 train_acc: 0.4195 train_loss: 1.361978\tval_acc: 0.651961 val_loss: 1.0338397\n",
      "2_12 train_acc: 0.4644 train_loss: 1.279434\tval_acc: 0.666667 val_loss: 1.0211295\n",
      "2_13 train_acc: 0.4494 train_loss: 1.340284\tval_acc: 0.661765 val_loss: 0.9931566\n",
      "2_14 train_acc: 0.4270 train_loss: 1.330113\tval_acc: 0.647059 val_loss: 0.9895563\n",
      "2_15 train_acc: 0.4682 train_loss: 1.267329\tval_acc: 0.700980 val_loss: 0.9704576\n",
      "2_17 train_acc: 0.4869 train_loss: 1.238633\tval_acc: 0.661765 val_loss: 0.9473299\n",
      "2_18 train_acc: 0.4794 train_loss: 1.282124\tval_acc: 0.656863 val_loss: 0.8945940\n",
      "2_21 train_acc: 0.5431 train_loss: 1.129695\tval_acc: 0.720588 val_loss: 0.8906679\n",
      "2_22 train_acc: 0.5281 train_loss: 1.129808\tval_acc: 0.754902 val_loss: 0.8505921\n",
      "2_24 train_acc: 0.5094 train_loss: 1.119598\tval_acc: 0.745098 val_loss: 0.8298166\n",
      "2_25 train_acc: 0.5356 train_loss: 1.112171\tval_acc: 0.754902 val_loss: 0.8067028\n",
      "2_28 train_acc: 0.5843 train_loss: 0.976889\tval_acc: 0.754902 val_loss: 0.8003148\n",
      "2_29 train_acc: 0.5094 train_loss: 1.086471\tval_acc: 0.769608 val_loss: 0.7873927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_30 train_acc: 0.5693 train_loss: 0.993822\tval_acc: 0.779412 val_loss: 0.7722383\n",
      "2_31 train_acc: 0.5543 train_loss: 1.009177\tval_acc: 0.759804 val_loss: 0.7513396\n",
      "2_33 train_acc: 0.5918 train_loss: 1.009498\tval_acc: 0.779412 val_loss: 0.7212336\n",
      "2_35 train_acc: 0.6180 train_loss: 0.933551\tval_acc: 0.764706 val_loss: 0.7078946\n",
      "2_36 train_acc: 0.6554 train_loss: 0.871794\tval_acc: 0.774510 val_loss: 0.6741776\n",
      "2_38 train_acc: 0.6180 train_loss: 0.969289\tval_acc: 0.828431 val_loss: 0.6347200\n",
      "2_41 train_acc: 0.5843 train_loss: 0.945853\tval_acc: 0.813725 val_loss: 0.6230605\n",
      "2_44 train_acc: 0.6255 train_loss: 0.904311\tval_acc: 0.818627 val_loss: 0.6127021\n",
      "2_46 train_acc: 0.6592 train_loss: 0.872641\tval_acc: 0.813725 val_loss: 0.5762656\n",
      "2_50 train_acc: 0.6629 train_loss: 0.850594\tval_acc: 0.818627 val_loss: 0.5724664\n",
      "2_52 train_acc: 0.6742 train_loss: 0.833364\tval_acc: 0.838235 val_loss: 0.5640951\n",
      "2_54 train_acc: 0.5993 train_loss: 0.928978\tval_acc: 0.833333 val_loss: 0.5452684\n",
      "2_55 train_acc: 0.5843 train_loss: 0.946702\tval_acc: 0.848039 val_loss: 0.5418218\n",
      "2_57 train_acc: 0.6367 train_loss: 0.865911\tval_acc: 0.838235 val_loss: 0.5361886\n",
      "2_62 train_acc: 0.6816 train_loss: 0.819400\tval_acc: 0.877451 val_loss: 0.4945849\n",
      "2_69 train_acc: 0.7079 train_loss: 0.761643\tval_acc: 0.867647 val_loss: 0.4835893\n",
      "2_74 train_acc: 0.7041 train_loss: 0.740300\tval_acc: 0.867647 val_loss: 0.4711310\n",
      "2_78 train_acc: 0.6292 train_loss: 0.892816\tval_acc: 0.848039 val_loss: 0.4687208\n",
      "2_82 train_acc: 0.6816 train_loss: 0.719534\tval_acc: 0.857843 val_loss: 0.4628924\n",
      "2_84 train_acc: 0.6816 train_loss: 0.752350\tval_acc: 0.872549 val_loss: 0.4494821\n",
      "2_87 train_acc: 0.6479 train_loss: 0.802985\tval_acc: 0.882353 val_loss: 0.4457428\n",
      "2_93 train_acc: 0.6816 train_loss: 0.748791\tval_acc: 0.867647 val_loss: 0.4434492\n",
      "2_95 train_acc: 0.7116 train_loss: 0.687525\tval_acc: 0.852941 val_loss: 0.4430764\n",
      "2_97 train_acc: 0.6742 train_loss: 0.805354\tval_acc: 0.887255 val_loss: 0.4425257\n",
      "2_102 train_acc: 0.7416 train_loss: 0.668149\tval_acc: 0.872549 val_loss: 0.4406265\n",
      "2_103 train_acc: 0.7266 train_loss: 0.717375\tval_acc: 0.872549 val_loss: 0.4324268\n",
      "2_104 train_acc: 0.6816 train_loss: 0.723388\tval_acc: 0.906863 val_loss: 0.3909056\n",
      "2_115 train_acc: 0.7079 train_loss: 0.668524\tval_acc: 0.892157 val_loss: 0.3840995\n",
      "2_120 train_acc: 0.7303 train_loss: 0.687301\tval_acc: 0.897059 val_loss: 0.3772530\n",
      "2_125 train_acc: 0.7041 train_loss: 0.767277\tval_acc: 0.906863 val_loss: 0.3743848\n",
      "2_130 train_acc: 0.6330 train_loss: 0.823519\tval_acc: 0.926471 val_loss: 0.3727711\n",
      "2_134 train_acc: 0.7041 train_loss: 0.709124\tval_acc: 0.916667 val_loss: 0.3617426\n",
      "2_142 train_acc: 0.7378 train_loss: 0.668901\tval_acc: 0.921569 val_loss: 0.3447987\n",
      "2_154 train_acc: 0.6966 train_loss: 0.728954\tval_acc: 0.901961 val_loss: 0.3442312\n",
      "2_157 train_acc: 0.7041 train_loss: 0.733614\tval_acc: 0.901961 val_loss: 0.3394009\n",
      "2_161 train_acc: 0.7528 train_loss: 0.614692\tval_acc: 0.931373 val_loss: 0.3309258\n",
      "2_163 train_acc: 0.7416 train_loss: 0.722270\tval_acc: 0.906863 val_loss: 0.3279676\n",
      "2_168 train_acc: 0.7865 train_loss: 0.573941\tval_acc: 0.911765 val_loss: 0.3187817\n",
      "2_174 train_acc: 0.7266 train_loss: 0.633876\tval_acc: 0.931373 val_loss: 0.3138917\n",
      "2_176 train_acc: 0.7341 train_loss: 0.756497\tval_acc: 0.946078 val_loss: 0.2886672\n",
      "2_189 train_acc: 0.7453 train_loss: 0.621388\tval_acc: 0.926471 val_loss: 0.2680160\n",
      "2_221 train_acc: 0.7116 train_loss: 0.693617\tval_acc: 0.936275 val_loss: 0.2638746\n",
      "2_226 train_acc: 0.7865 train_loss: 0.558278\tval_acc: 0.926471 val_loss: 0.2595477\n",
      "2_233 train_acc: 0.7416 train_loss: 0.582863\tval_acc: 0.936275 val_loss: 0.2503406\n",
      "2_238 train_acc: 0.7715 train_loss: 0.632152\tval_acc: 0.941176 val_loss: 0.2381587\n",
      "2_241 train_acc: 0.7940 train_loss: 0.598452\tval_acc: 0.936275 val_loss: 0.2304965\n",
      "2_244 train_acc: 0.7603 train_loss: 0.599782\tval_acc: 0.960784 val_loss: 0.2226083\n",
      "2_247 train_acc: 0.7828 train_loss: 0.529631\tval_acc: 0.950980 val_loss: 0.2208259\n",
      "2_248 train_acc: 0.7978 train_loss: 0.512838\tval_acc: 0.955882 val_loss: 0.2037740\n",
      "2_255 train_acc: 0.7790 train_loss: 0.531235\tval_acc: 0.950980 val_loss: 0.2026200\n",
      "2_256 train_acc: 0.8614 train_loss: 0.409818\tval_acc: 0.955882 val_loss: 0.1914520\n",
      "2_274 train_acc: 0.8577 train_loss: 0.404903\tval_acc: 0.950980 val_loss: 0.1888669\n",
      "2_283 train_acc: 0.8315 train_loss: 0.460656\tval_acc: 0.936275 val_loss: 0.1873054\n",
      "2_291 train_acc: 0.8127 train_loss: 0.493201\tval_acc: 0.960784 val_loss: 0.1623316\n",
      "2_295 train_acc: 0.8202 train_loss: 0.472317\tval_acc: 0.985294 val_loss: 0.1249574\n",
      "2_336 train_acc: 0.8689 train_loss: 0.354924\tval_acc: 0.975490 val_loss: 0.1063349\n",
      "2_398 train_acc: 0.8464 train_loss: 0.347531\tval_acc: 0.975490 val_loss: 0.1060150\n",
      "2_409 train_acc: 0.8315 train_loss: 0.440361\tval_acc: 0.985294 val_loss: 0.1017595\n",
      "2_411 train_acc: 0.8577 train_loss: 0.366663\tval_acc: 0.985294 val_loss: 0.1004858\n",
      "2_416 train_acc: 0.8464 train_loss: 0.396952\tval_acc: 0.975490 val_loss: 0.0877413\n",
      "2_435 train_acc: 0.8876 train_loss: 0.336810\tval_acc: 0.990196 val_loss: 0.0874989\n",
      "2_483 train_acc: 0.8764 train_loss: 0.332754\tval_acc: 0.990196 val_loss: 0.0825385\n",
      "2_487 train_acc: 0.8951 train_loss: 0.291274\tval_acc: 0.990196 val_loss: 0.0790682\n",
      "2_494 train_acc: 0.8801 train_loss: 0.368141\tval_acc: 0.980392 val_loss: 0.0772496\n",
      "2_529 train_acc: 0.8989 train_loss: 0.254622\tval_acc: 0.975490 val_loss: 0.0770033\n",
      "2_552 train_acc: 0.8502 train_loss: 0.364526\tval_acc: 0.990196 val_loss: 0.0743206\n",
      "2_555 train_acc: 0.9101 train_loss: 0.279897\tval_acc: 0.995098 val_loss: 0.0675584\n",
      "2_578 train_acc: 0.8801 train_loss: 0.327095\tval_acc: 0.995098 val_loss: 0.0618266\n",
      "2_606 train_acc: 0.8839 train_loss: 0.339963\tval_acc: 0.995098 val_loss: 0.0572570\n",
      "2_634 train_acc: 0.8727 train_loss: 0.334213\tval_acc: 0.985294 val_loss: 0.0566302\n",
      "2_660 train_acc: 0.9176 train_loss: 0.246759\tval_acc: 1.000000 val_loss: 0.0478121\n",
      "2_681 train_acc: 0.8951 train_loss: 0.307968\tval_acc: 1.000000 val_loss: 0.0435031\n",
      "2_731 train_acc: 0.8577 train_loss: 0.378441\tval_acc: 1.000000 val_loss: 0.0421294\n",
      "2_771 train_acc: 0.8989 train_loss: 0.301418\tval_acc: 1.000000 val_loss: 0.0371159\n",
      "2_813 train_acc: 0.9064 train_loss: 0.235014\tval_acc: 0.995098 val_loss: 0.0329626\n",
      "2_825 train_acc: 0.8839 train_loss: 0.298438\tval_acc: 1.000000 val_loss: 0.0324396\n",
      "2_873 train_acc: 0.9139 train_loss: 0.255341\tval_acc: 0.995098 val_loss: 0.0320340\n",
      "2_883 train_acc: 0.8951 train_loss: 0.285845\tval_acc: 1.000000 val_loss: 0.0260615\n",
      "2_974 train_acc: 0.9101 train_loss: 0.248928\tval_acc: 1.000000 val_loss: 0.0209420\n",
      "epoch:  974 \tThe test accuracy is: 0.7291666666666666\n",
      " THE BEST ACCURACY IS 0.7291666666666666\tkappa is 0.6388888888888888\n",
      "subject 2 duration: 0:20:05.893950\n",
      "seed is 1880\n",
      "Subject 3\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "3_0 train_acc: 0.2697 train_loss: 2.414391\tval_acc: 0.308824 val_loss: 1.3639078\n",
      "3_1 train_acc: 0.3371 train_loss: 2.029040\tval_acc: 0.343137 val_loss: 1.3313702\n",
      "3_2 train_acc: 0.3258 train_loss: 1.797847\tval_acc: 0.377451 val_loss: 1.3063519\n",
      "3_3 train_acc: 0.3596 train_loss: 1.733828\tval_acc: 0.367647 val_loss: 1.2824473\n",
      "3_4 train_acc: 0.3258 train_loss: 1.795484\tval_acc: 0.406863 val_loss: 1.2259156\n",
      "3_5 train_acc: 0.3521 train_loss: 1.566122\tval_acc: 0.470588 val_loss: 1.1833580\n",
      "3_6 train_acc: 0.3670 train_loss: 1.639396\tval_acc: 0.573529 val_loss: 1.1072164\n",
      "3_7 train_acc: 0.4195 train_loss: 1.412878\tval_acc: 0.568627 val_loss: 1.0255200\n",
      "3_8 train_acc: 0.5019 train_loss: 1.247543\tval_acc: 0.622549 val_loss: 0.9707642\n",
      "3_9 train_acc: 0.4494 train_loss: 1.319661\tval_acc: 0.651961 val_loss: 0.9456380\n",
      "3_10 train_acc: 0.4607 train_loss: 1.207734\tval_acc: 0.622549 val_loss: 0.9288430\n",
      "3_11 train_acc: 0.4719 train_loss: 1.279000\tval_acc: 0.700980 val_loss: 0.8607095\n",
      "3_12 train_acc: 0.4682 train_loss: 1.215238\tval_acc: 0.735294 val_loss: 0.8066438\n",
      "3_14 train_acc: 0.5281 train_loss: 1.180580\tval_acc: 0.720588 val_loss: 0.7671270\n",
      "3_15 train_acc: 0.5506 train_loss: 1.097166\tval_acc: 0.705882 val_loss: 0.7551690\n",
      "3_17 train_acc: 0.5506 train_loss: 1.093501\tval_acc: 0.759804 val_loss: 0.6806386\n",
      "3_21 train_acc: 0.6030 train_loss: 0.935824\tval_acc: 0.794118 val_loss: 0.6199539\n",
      "3_22 train_acc: 0.6217 train_loss: 0.947599\tval_acc: 0.779412 val_loss: 0.6058505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_23 train_acc: 0.6180 train_loss: 0.954799\tval_acc: 0.794118 val_loss: 0.5971347\n",
      "3_28 train_acc: 0.6742 train_loss: 0.812142\tval_acc: 0.799020 val_loss: 0.5860439\n",
      "3_29 train_acc: 0.6742 train_loss: 0.800904\tval_acc: 0.784314 val_loss: 0.5835966\n",
      "3_30 train_acc: 0.6629 train_loss: 0.787365\tval_acc: 0.808824 val_loss: 0.5331498\n",
      "3_36 train_acc: 0.6891 train_loss: 0.801861\tval_acc: 0.823529 val_loss: 0.4819491\n",
      "3_39 train_acc: 0.7154 train_loss: 0.776719\tval_acc: 0.843137 val_loss: 0.4503557\n",
      "3_44 train_acc: 0.7191 train_loss: 0.703306\tval_acc: 0.852941 val_loss: 0.4172957\n",
      "3_46 train_acc: 0.7341 train_loss: 0.683137\tval_acc: 0.862745 val_loss: 0.4033098\n",
      "3_51 train_acc: 0.6966 train_loss: 0.712021\tval_acc: 0.877451 val_loss: 0.3869971\n",
      "3_52 train_acc: 0.7416 train_loss: 0.605647\tval_acc: 0.867647 val_loss: 0.3639707\n",
      "3_65 train_acc: 0.7978 train_loss: 0.482566\tval_acc: 0.862745 val_loss: 0.3441083\n",
      "3_66 train_acc: 0.7715 train_loss: 0.514149\tval_acc: 0.892157 val_loss: 0.3112175\n",
      "3_67 train_acc: 0.8052 train_loss: 0.493575\tval_acc: 0.906863 val_loss: 0.2517467\n",
      "3_75 train_acc: 0.8427 train_loss: 0.406659\tval_acc: 0.950980 val_loss: 0.2012834\n",
      "3_79 train_acc: 0.8539 train_loss: 0.383901\tval_acc: 0.965686 val_loss: 0.1696627\n",
      "3_81 train_acc: 0.8689 train_loss: 0.328311\tval_acc: 0.946078 val_loss: 0.1675806\n",
      "3_85 train_acc: 0.8689 train_loss: 0.332990\tval_acc: 0.965686 val_loss: 0.1615823\n",
      "3_91 train_acc: 0.8951 train_loss: 0.272288\tval_acc: 0.950980 val_loss: 0.1494417\n",
      "3_94 train_acc: 0.8764 train_loss: 0.339225\tval_acc: 0.960784 val_loss: 0.1484455\n",
      "3_101 train_acc: 0.8801 train_loss: 0.347908\tval_acc: 0.955882 val_loss: 0.1450635\n",
      "3_103 train_acc: 0.8839 train_loss: 0.372640\tval_acc: 0.950980 val_loss: 0.1253520\n",
      "3_111 train_acc: 0.8689 train_loss: 0.348063\tval_acc: 0.965686 val_loss: 0.1011330\n",
      "3_129 train_acc: 0.8951 train_loss: 0.295042\tval_acc: 0.955882 val_loss: 0.0996276\n",
      "3_134 train_acc: 0.8839 train_loss: 0.263690\tval_acc: 0.975490 val_loss: 0.0957573\n",
      "3_135 train_acc: 0.9176 train_loss: 0.202961\tval_acc: 0.975490 val_loss: 0.0898165\n",
      "3_149 train_acc: 0.9401 train_loss: 0.197999\tval_acc: 0.980392 val_loss: 0.0810083\n",
      "3_158 train_acc: 0.8764 train_loss: 0.241715\tval_acc: 0.970588 val_loss: 0.0787859\n",
      "3_168 train_acc: 0.9176 train_loss: 0.236809\tval_acc: 0.985294 val_loss: 0.0633523\n",
      "3_210 train_acc: 0.9251 train_loss: 0.209549\tval_acc: 0.985294 val_loss: 0.0624128\n",
      "3_218 train_acc: 0.9326 train_loss: 0.139602\tval_acc: 0.980392 val_loss: 0.0555484\n",
      "3_221 train_acc: 0.9288 train_loss: 0.219147\tval_acc: 0.985294 val_loss: 0.0511073\n",
      "3_247 train_acc: 0.9438 train_loss: 0.171608\tval_acc: 0.980392 val_loss: 0.0506434\n",
      "3_248 train_acc: 0.9363 train_loss: 0.169734\tval_acc: 0.980392 val_loss: 0.0483250\n",
      "3_267 train_acc: 0.9438 train_loss: 0.137881\tval_acc: 0.985294 val_loss: 0.0473969\n",
      "3_268 train_acc: 0.9288 train_loss: 0.196811\tval_acc: 0.980392 val_loss: 0.0447305\n",
      "3_274 train_acc: 0.9551 train_loss: 0.109046\tval_acc: 0.985294 val_loss: 0.0410521\n",
      "3_291 train_acc: 0.9551 train_loss: 0.144275\tval_acc: 0.985294 val_loss: 0.0324709\n",
      "3_304 train_acc: 0.9625 train_loss: 0.107379\tval_acc: 1.000000 val_loss: 0.0210058\n",
      "3_330 train_acc: 0.9476 train_loss: 0.131984\tval_acc: 0.995098 val_loss: 0.0195349\n",
      "3_360 train_acc: 0.9700 train_loss: 0.099856\tval_acc: 0.995098 val_loss: 0.0186273\n",
      "3_401 train_acc: 0.9363 train_loss: 0.137522\tval_acc: 1.000000 val_loss: 0.0155067\n",
      "3_419 train_acc: 0.9738 train_loss: 0.066364\tval_acc: 1.000000 val_loss: 0.0145570\n",
      "3_440 train_acc: 0.9476 train_loss: 0.134577\tval_acc: 0.995098 val_loss: 0.0144655\n",
      "3_449 train_acc: 0.9775 train_loss: 0.090179\tval_acc: 1.000000 val_loss: 0.0109062\n",
      "3_471 train_acc: 0.9625 train_loss: 0.138894\tval_acc: 1.000000 val_loss: 0.0106536\n",
      "3_532 train_acc: 0.9588 train_loss: 0.100071\tval_acc: 1.000000 val_loss: 0.0057437\n",
      "3_635 train_acc: 0.9625 train_loss: 0.116362\tval_acc: 1.000000 val_loss: 0.0055981\n",
      "3_651 train_acc: 0.9551 train_loss: 0.147035\tval_acc: 1.000000 val_loss: 0.0050948\n",
      "3_668 train_acc: 0.9663 train_loss: 0.105603\tval_acc: 1.000000 val_loss: 0.0049174\n",
      "3_671 train_acc: 0.9663 train_loss: 0.078234\tval_acc: 1.000000 val_loss: 0.0042445\n",
      "3_703 train_acc: 0.9813 train_loss: 0.054467\tval_acc: 1.000000 val_loss: 0.0038555\n",
      "3_755 train_acc: 0.9775 train_loss: 0.059657\tval_acc: 1.000000 val_loss: 0.0031405\n",
      "3_798 train_acc: 0.9738 train_loss: 0.060485\tval_acc: 1.000000 val_loss: 0.0031234\n",
      "3_802 train_acc: 0.9700 train_loss: 0.067013\tval_acc: 1.000000 val_loss: 0.0026028\n",
      "3_849 train_acc: 0.9551 train_loss: 0.109400\tval_acc: 1.000000 val_loss: 0.0012640\n",
      "3_981 train_acc: 0.9663 train_loss: 0.066457\tval_acc: 1.000000 val_loss: 0.0012067\n",
      "epoch:  981 \tThe test accuracy is: 0.9201388888888888\n",
      " THE BEST ACCURACY IS 0.9201388888888888\tkappa is 0.8935185185185185\n",
      "subject 3 duration: 0:20:01.417362\n",
      "seed is 1873\n",
      "Subject 4\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "4_0 train_acc: 0.2772 train_loss: 2.225777\tval_acc: 0.250000 val_loss: 1.4853045\n",
      "4_1 train_acc: 0.2584 train_loss: 2.219215\tval_acc: 0.284314 val_loss: 1.4037269\n",
      "4_2 train_acc: 0.3184 train_loss: 1.957499\tval_acc: 0.294118 val_loss: 1.3576977\n",
      "4_3 train_acc: 0.3483 train_loss: 1.795212\tval_acc: 0.338235 val_loss: 1.2914430\n",
      "4_5 train_acc: 0.3708 train_loss: 1.598292\tval_acc: 0.539216 val_loss: 1.1477156\n",
      "4_6 train_acc: 0.3708 train_loss: 1.547079\tval_acc: 0.539216 val_loss: 1.1143543\n",
      "4_7 train_acc: 0.3633 train_loss: 1.559646\tval_acc: 0.578431 val_loss: 1.0720551\n",
      "4_8 train_acc: 0.3708 train_loss: 1.468381\tval_acc: 0.607843 val_loss: 1.0357171\n",
      "4_9 train_acc: 0.3970 train_loss: 1.374938\tval_acc: 0.622549 val_loss: 1.0235709\n",
      "4_10 train_acc: 0.4045 train_loss: 1.355861\tval_acc: 0.676471 val_loss: 0.9564500\n",
      "4_13 train_acc: 0.4569 train_loss: 1.222926\tval_acc: 0.681373 val_loss: 0.9102860\n",
      "4_16 train_acc: 0.5094 train_loss: 1.120226\tval_acc: 0.759804 val_loss: 0.8240018\n",
      "4_18 train_acc: 0.4944 train_loss: 1.197292\tval_acc: 0.745098 val_loss: 0.8089069\n",
      "4_22 train_acc: 0.5805 train_loss: 1.030619\tval_acc: 0.769608 val_loss: 0.7964170\n",
      "4_24 train_acc: 0.5356 train_loss: 1.047480\tval_acc: 0.735294 val_loss: 0.7550897\n",
      "4_25 train_acc: 0.6217 train_loss: 0.941623\tval_acc: 0.730392 val_loss: 0.7357811\n",
      "4_28 train_acc: 0.5955 train_loss: 1.059814\tval_acc: 0.764706 val_loss: 0.7162735\n",
      "4_30 train_acc: 0.6180 train_loss: 0.890824\tval_acc: 0.789216 val_loss: 0.6733297\n",
      "4_33 train_acc: 0.6330 train_loss: 0.907402\tval_acc: 0.808824 val_loss: 0.6580722\n",
      "4_36 train_acc: 0.6067 train_loss: 0.957685\tval_acc: 0.764706 val_loss: 0.6562030\n",
      "4_37 train_acc: 0.6929 train_loss: 0.810324\tval_acc: 0.813725 val_loss: 0.6015608\n",
      "4_39 train_acc: 0.6479 train_loss: 0.854636\tval_acc: 0.833333 val_loss: 0.5933733\n",
      "4_41 train_acc: 0.6404 train_loss: 0.918462\tval_acc: 0.823529 val_loss: 0.5911257\n",
      "4_42 train_acc: 0.6404 train_loss: 0.920848\tval_acc: 0.828431 val_loss: 0.5697920\n",
      "4_48 train_acc: 0.6592 train_loss: 0.856531\tval_acc: 0.823529 val_loss: 0.5516998\n",
      "4_51 train_acc: 0.6629 train_loss: 0.859174\tval_acc: 0.794118 val_loss: 0.5480941\n",
      "4_53 train_acc: 0.6704 train_loss: 0.840830\tval_acc: 0.833333 val_loss: 0.5099310\n",
      "4_54 train_acc: 0.6479 train_loss: 0.817051\tval_acc: 0.848039 val_loss: 0.5091555\n",
      "4_58 train_acc: 0.6891 train_loss: 0.757328\tval_acc: 0.848039 val_loss: 0.5010529\n",
      "4_60 train_acc: 0.6667 train_loss: 0.837904\tval_acc: 0.862745 val_loss: 0.4729302\n",
      "4_65 train_acc: 0.6891 train_loss: 0.755459\tval_acc: 0.828431 val_loss: 0.4550543\n",
      "4_68 train_acc: 0.6704 train_loss: 0.768300\tval_acc: 0.892157 val_loss: 0.4138451\n",
      "4_78 train_acc: 0.6517 train_loss: 0.826967\tval_acc: 0.862745 val_loss: 0.4097665\n",
      "4_80 train_acc: 0.7940 train_loss: 0.576621\tval_acc: 0.867647 val_loss: 0.3964090\n",
      "4_84 train_acc: 0.7041 train_loss: 0.725327\tval_acc: 0.862745 val_loss: 0.3911225\n",
      "4_87 train_acc: 0.6891 train_loss: 0.781997\tval_acc: 0.877451 val_loss: 0.3870463\n",
      "4_88 train_acc: 0.7266 train_loss: 0.681421\tval_acc: 0.887255 val_loss: 0.3706398\n",
      "4_94 train_acc: 0.7191 train_loss: 0.714795\tval_acc: 0.897059 val_loss: 0.3651243\n",
      "4_98 train_acc: 0.7491 train_loss: 0.639811\tval_acc: 0.916667 val_loss: 0.3217712\n",
      "4_105 train_acc: 0.7453 train_loss: 0.636516\tval_acc: 0.892157 val_loss: 0.3197929\n",
      "4_111 train_acc: 0.7528 train_loss: 0.618188\tval_acc: 0.906863 val_loss: 0.3090030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_115 train_acc: 0.7528 train_loss: 0.646819\tval_acc: 0.887255 val_loss: 0.3070424\n",
      "4_121 train_acc: 0.8052 train_loss: 0.559751\tval_acc: 0.897059 val_loss: 0.2935484\n",
      "4_123 train_acc: 0.7828 train_loss: 0.555073\tval_acc: 0.911765 val_loss: 0.2908670\n",
      "4_127 train_acc: 0.7603 train_loss: 0.603969\tval_acc: 0.916667 val_loss: 0.2789775\n",
      "4_134 train_acc: 0.7678 train_loss: 0.603673\tval_acc: 0.921569 val_loss: 0.2741978\n",
      "4_135 train_acc: 0.7678 train_loss: 0.568837\tval_acc: 0.926471 val_loss: 0.2616286\n",
      "4_137 train_acc: 0.7978 train_loss: 0.570422\tval_acc: 0.921569 val_loss: 0.2464073\n",
      "4_148 train_acc: 0.7903 train_loss: 0.568547\tval_acc: 0.931373 val_loss: 0.2425217\n",
      "4_159 train_acc: 0.7940 train_loss: 0.518425\tval_acc: 0.921569 val_loss: 0.2417615\n",
      "4_169 train_acc: 0.8015 train_loss: 0.538417\tval_acc: 0.946078 val_loss: 0.2412428\n",
      "4_170 train_acc: 0.7715 train_loss: 0.581387\tval_acc: 0.931373 val_loss: 0.2339395\n",
      "4_173 train_acc: 0.7341 train_loss: 0.612290\tval_acc: 0.946078 val_loss: 0.2122555\n",
      "4_185 train_acc: 0.7865 train_loss: 0.541879\tval_acc: 0.946078 val_loss: 0.2080550\n",
      "4_188 train_acc: 0.7753 train_loss: 0.554801\tval_acc: 0.926471 val_loss: 0.2068037\n",
      "4_191 train_acc: 0.8352 train_loss: 0.441834\tval_acc: 0.941176 val_loss: 0.2046091\n",
      "4_194 train_acc: 0.7940 train_loss: 0.539461\tval_acc: 0.941176 val_loss: 0.1891478\n",
      "4_204 train_acc: 0.8277 train_loss: 0.445010\tval_acc: 0.936275 val_loss: 0.1879495\n",
      "4_211 train_acc: 0.7940 train_loss: 0.573615\tval_acc: 0.950980 val_loss: 0.1849584\n",
      "4_212 train_acc: 0.8052 train_loss: 0.540700\tval_acc: 0.946078 val_loss: 0.1815574\n",
      "4_214 train_acc: 0.8052 train_loss: 0.510568\tval_acc: 0.946078 val_loss: 0.1750151\n",
      "4_227 train_acc: 0.8165 train_loss: 0.484662\tval_acc: 0.955882 val_loss: 0.1647791\n",
      "4_238 train_acc: 0.8427 train_loss: 0.395981\tval_acc: 0.960784 val_loss: 0.1539991\n",
      "4_242 train_acc: 0.8240 train_loss: 0.453117\tval_acc: 0.965686 val_loss: 0.1418040\n",
      "4_256 train_acc: 0.8240 train_loss: 0.382203\tval_acc: 0.980392 val_loss: 0.1207161\n",
      "4_310 train_acc: 0.8614 train_loss: 0.386432\tval_acc: 0.955882 val_loss: 0.1189748\n",
      "4_315 train_acc: 0.8614 train_loss: 0.363099\tval_acc: 0.970588 val_loss: 0.1171948\n",
      "4_321 train_acc: 0.8539 train_loss: 0.405758\tval_acc: 0.975490 val_loss: 0.1033452\n",
      "4_329 train_acc: 0.8689 train_loss: 0.368225\tval_acc: 0.995098 val_loss: 0.0865008\n",
      "4_354 train_acc: 0.8427 train_loss: 0.355055\tval_acc: 0.985294 val_loss: 0.0864221\n",
      "4_368 train_acc: 0.8801 train_loss: 0.406598\tval_acc: 0.985294 val_loss: 0.0839501\n",
      "4_369 train_acc: 0.8240 train_loss: 0.480702\tval_acc: 0.985294 val_loss: 0.0810122\n",
      "4_381 train_acc: 0.8427 train_loss: 0.386112\tval_acc: 0.980392 val_loss: 0.0755553\n",
      "4_403 train_acc: 0.8464 train_loss: 0.390553\tval_acc: 0.985294 val_loss: 0.0702628\n",
      "4_423 train_acc: 0.8277 train_loss: 0.414610\tval_acc: 0.985294 val_loss: 0.0672201\n",
      "4_454 train_acc: 0.8727 train_loss: 0.353323\tval_acc: 0.995098 val_loss: 0.0658296\n",
      "4_465 train_acc: 0.8876 train_loss: 0.340301\tval_acc: 0.995098 val_loss: 0.0617043\n",
      "4_468 train_acc: 0.8801 train_loss: 0.323921\tval_acc: 0.990196 val_loss: 0.0590666\n",
      "4_471 train_acc: 0.8614 train_loss: 0.418161\tval_acc: 0.990196 val_loss: 0.0532576\n",
      "4_513 train_acc: 0.8914 train_loss: 0.289583\tval_acc: 0.990196 val_loss: 0.0496890\n",
      "4_570 train_acc: 0.8951 train_loss: 0.272826\tval_acc: 0.990196 val_loss: 0.0480391\n",
      "4_578 train_acc: 0.9101 train_loss: 0.318380\tval_acc: 1.000000 val_loss: 0.0462424\n",
      "4_586 train_acc: 0.9064 train_loss: 0.287951\tval_acc: 0.995098 val_loss: 0.0455007\n",
      "4_594 train_acc: 0.8764 train_loss: 0.300953\tval_acc: 0.990196 val_loss: 0.0442209\n",
      "4_597 train_acc: 0.9026 train_loss: 0.258593\tval_acc: 1.000000 val_loss: 0.0433413\n",
      "4_609 train_acc: 0.9064 train_loss: 0.229543\tval_acc: 0.995098 val_loss: 0.0431220\n",
      "4_632 train_acc: 0.8764 train_loss: 0.340652\tval_acc: 1.000000 val_loss: 0.0397908\n",
      "4_661 train_acc: 0.9026 train_loss: 0.309512\tval_acc: 0.990196 val_loss: 0.0396741\n",
      "4_665 train_acc: 0.8839 train_loss: 0.356254\tval_acc: 1.000000 val_loss: 0.0306276\n",
      "4_737 train_acc: 0.8876 train_loss: 0.281291\tval_acc: 0.995098 val_loss: 0.0279288\n",
      "4_762 train_acc: 0.9064 train_loss: 0.268421\tval_acc: 0.995098 val_loss: 0.0279192\n",
      "4_771 train_acc: 0.8951 train_loss: 0.304071\tval_acc: 0.995098 val_loss: 0.0262007\n",
      "4_773 train_acc: 0.8951 train_loss: 0.273561\tval_acc: 1.000000 val_loss: 0.0261296\n",
      "4_791 train_acc: 0.8876 train_loss: 0.280089\tval_acc: 0.995098 val_loss: 0.0258296\n",
      "4_808 train_acc: 0.8839 train_loss: 0.273908\tval_acc: 0.995098 val_loss: 0.0239777\n",
      "4_827 train_acc: 0.8989 train_loss: 0.268474\tval_acc: 1.000000 val_loss: 0.0209977\n",
      "4_862 train_acc: 0.9139 train_loss: 0.240558\tval_acc: 1.000000 val_loss: 0.0198000\n",
      "4_915 train_acc: 0.8951 train_loss: 0.271900\tval_acc: 1.000000 val_loss: 0.0197843\n",
      "4_919 train_acc: 0.9176 train_loss: 0.190305\tval_acc: 1.000000 val_loss: 0.0164143\n",
      "4_978 train_acc: 0.9326 train_loss: 0.251302\tval_acc: 1.000000 val_loss: 0.0139763\n",
      "epoch:  978 \tThe test accuracy is: 0.8298611111111112\n",
      " THE BEST ACCURACY IS 0.8298611111111112\tkappa is 0.7731481481481481\n",
      "subject 4 duration: 0:20:07.884473\n",
      "seed is 46\n",
      "Subject 5\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "5_0 train_acc: 0.2659 train_loss: 2.539057\tval_acc: 0.235294 val_loss: 1.8348349\n",
      "5_1 train_acc: 0.2846 train_loss: 2.327380\tval_acc: 0.269608 val_loss: 1.4751444\n",
      "5_2 train_acc: 0.3221 train_loss: 2.073906\tval_acc: 0.294118 val_loss: 1.3713151\n",
      "5_3 train_acc: 0.3895 train_loss: 1.895590\tval_acc: 0.421569 val_loss: 1.2344368\n",
      "5_4 train_acc: 0.3596 train_loss: 1.797295\tval_acc: 0.534314 val_loss: 1.0920800\n",
      "5_5 train_acc: 0.3783 train_loss: 1.557098\tval_acc: 0.578431 val_loss: 1.0566075\n",
      "5_6 train_acc: 0.4120 train_loss: 1.462862\tval_acc: 0.637255 val_loss: 0.9805903\n",
      "5_7 train_acc: 0.4906 train_loss: 1.333097\tval_acc: 0.666667 val_loss: 0.8770576\n",
      "5_8 train_acc: 0.4082 train_loss: 1.397568\tval_acc: 0.730392 val_loss: 0.8205830\n",
      "5_9 train_acc: 0.4607 train_loss: 1.383732\tval_acc: 0.715686 val_loss: 0.8205441\n",
      "5_10 train_acc: 0.4569 train_loss: 1.443222\tval_acc: 0.774510 val_loss: 0.7360287\n",
      "5_11 train_acc: 0.5506 train_loss: 1.105600\tval_acc: 0.784314 val_loss: 0.6965330\n",
      "5_12 train_acc: 0.5693 train_loss: 1.009280\tval_acc: 0.779412 val_loss: 0.6660320\n",
      "5_13 train_acc: 0.5431 train_loss: 1.123453\tval_acc: 0.799020 val_loss: 0.6562936\n",
      "5_14 train_acc: 0.5169 train_loss: 1.162484\tval_acc: 0.764706 val_loss: 0.6468923\n",
      "5_15 train_acc: 0.6217 train_loss: 0.856261\tval_acc: 0.813725 val_loss: 0.6126622\n",
      "5_16 train_acc: 0.6292 train_loss: 0.967465\tval_acc: 0.813725 val_loss: 0.5954072\n",
      "5_17 train_acc: 0.6479 train_loss: 0.910713\tval_acc: 0.789216 val_loss: 0.5863537\n",
      "5_18 train_acc: 0.6105 train_loss: 0.941710\tval_acc: 0.833333 val_loss: 0.5501861\n",
      "5_20 train_acc: 0.5918 train_loss: 0.982253\tval_acc: 0.813725 val_loss: 0.5291368\n",
      "5_22 train_acc: 0.6217 train_loss: 0.871029\tval_acc: 0.843137 val_loss: 0.5122325\n",
      "5_23 train_acc: 0.6667 train_loss: 0.843612\tval_acc: 0.828431 val_loss: 0.4714803\n",
      "5_27 train_acc: 0.6779 train_loss: 0.782453\tval_acc: 0.877451 val_loss: 0.4504579\n",
      "5_31 train_acc: 0.7079 train_loss: 0.772676\tval_acc: 0.862745 val_loss: 0.4494515\n",
      "5_32 train_acc: 0.6929 train_loss: 0.679568\tval_acc: 0.848039 val_loss: 0.4374570\n",
      "5_34 train_acc: 0.7453 train_loss: 0.676394\tval_acc: 0.887255 val_loss: 0.4071464\n",
      "5_38 train_acc: 0.6816 train_loss: 0.715678\tval_acc: 0.877451 val_loss: 0.3917904\n",
      "5_40 train_acc: 0.7903 train_loss: 0.559894\tval_acc: 0.877451 val_loss: 0.3810262\n",
      "5_43 train_acc: 0.7378 train_loss: 0.682025\tval_acc: 0.877451 val_loss: 0.3700484\n",
      "5_49 train_acc: 0.7566 train_loss: 0.643525\tval_acc: 0.897059 val_loss: 0.3362822\n",
      "5_55 train_acc: 0.8127 train_loss: 0.518719\tval_acc: 0.911765 val_loss: 0.3223712\n",
      "5_58 train_acc: 0.7491 train_loss: 0.648791\tval_acc: 0.916667 val_loss: 0.2974103\n",
      "5_66 train_acc: 0.8390 train_loss: 0.484820\tval_acc: 0.897059 val_loss: 0.2894930\n",
      "5_76 train_acc: 0.7790 train_loss: 0.585149\tval_acc: 0.911765 val_loss: 0.2881124\n",
      "5_79 train_acc: 0.7828 train_loss: 0.586260\tval_acc: 0.926471 val_loss: 0.2732092\n",
      "5_80 train_acc: 0.8202 train_loss: 0.481692\tval_acc: 0.921569 val_loss: 0.2626916\n",
      "5_86 train_acc: 0.8052 train_loss: 0.510017\tval_acc: 0.926471 val_loss: 0.2493239\n",
      "5_87 train_acc: 0.8127 train_loss: 0.462006\tval_acc: 0.941176 val_loss: 0.2478624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_88 train_acc: 0.8165 train_loss: 0.485470\tval_acc: 0.921569 val_loss: 0.2457034\n",
      "5_90 train_acc: 0.7865 train_loss: 0.511651\tval_acc: 0.921569 val_loss: 0.2424754\n",
      "5_93 train_acc: 0.7865 train_loss: 0.559664\tval_acc: 0.931373 val_loss: 0.2129847\n",
      "5_100 train_acc: 0.8464 train_loss: 0.382793\tval_acc: 0.936275 val_loss: 0.2060985\n",
      "5_109 train_acc: 0.8277 train_loss: 0.430944\tval_acc: 0.941176 val_loss: 0.2027691\n",
      "5_114 train_acc: 0.8315 train_loss: 0.413532\tval_acc: 0.936275 val_loss: 0.1957652\n",
      "5_116 train_acc: 0.8202 train_loss: 0.478459\tval_acc: 0.950980 val_loss: 0.1938128\n",
      "5_126 train_acc: 0.8502 train_loss: 0.428709\tval_acc: 0.950980 val_loss: 0.1935576\n",
      "5_129 train_acc: 0.8202 train_loss: 0.538778\tval_acc: 0.965686 val_loss: 0.1740549\n",
      "5_130 train_acc: 0.8277 train_loss: 0.449379\tval_acc: 0.960784 val_loss: 0.1691879\n",
      "5_140 train_acc: 0.8052 train_loss: 0.450481\tval_acc: 0.965686 val_loss: 0.1690423\n",
      "5_142 train_acc: 0.8539 train_loss: 0.399115\tval_acc: 0.946078 val_loss: 0.1653057\n",
      "5_144 train_acc: 0.8352 train_loss: 0.476156\tval_acc: 0.950980 val_loss: 0.1561947\n",
      "5_163 train_acc: 0.8577 train_loss: 0.348117\tval_acc: 0.955882 val_loss: 0.1468606\n",
      "5_166 train_acc: 0.8277 train_loss: 0.455751\tval_acc: 0.965686 val_loss: 0.1445615\n",
      "5_173 train_acc: 0.8277 train_loss: 0.482878\tval_acc: 0.965686 val_loss: 0.1288381\n",
      "5_174 train_acc: 0.8539 train_loss: 0.399031\tval_acc: 0.985294 val_loss: 0.1145060\n",
      "5_175 train_acc: 0.8689 train_loss: 0.356631\tval_acc: 0.970588 val_loss: 0.1141822\n",
      "5_217 train_acc: 0.8165 train_loss: 0.424227\tval_acc: 0.975490 val_loss: 0.1115099\n",
      "5_226 train_acc: 0.8464 train_loss: 0.383251\tval_acc: 0.965686 val_loss: 0.1047941\n",
      "5_265 train_acc: 0.8614 train_loss: 0.385074\tval_acc: 0.985294 val_loss: 0.0999750\n",
      "5_266 train_acc: 0.8464 train_loss: 0.414141\tval_acc: 0.975490 val_loss: 0.0985809\n",
      "5_268 train_acc: 0.8577 train_loss: 0.405953\tval_acc: 0.975490 val_loss: 0.0928965\n",
      "5_274 train_acc: 0.8614 train_loss: 0.340206\tval_acc: 0.975490 val_loss: 0.0883046\n",
      "5_291 train_acc: 0.8165 train_loss: 0.432698\tval_acc: 0.980392 val_loss: 0.0829608\n",
      "5_337 train_acc: 0.8914 train_loss: 0.359438\tval_acc: 0.980392 val_loss: 0.0811020\n",
      "5_358 train_acc: 0.8727 train_loss: 0.374303\tval_acc: 0.980392 val_loss: 0.0799745\n",
      "5_359 train_acc: 0.8614 train_loss: 0.357764\tval_acc: 0.970588 val_loss: 0.0744738\n",
      "5_378 train_acc: 0.8652 train_loss: 0.348509\tval_acc: 0.985294 val_loss: 0.0729796\n",
      "5_382 train_acc: 0.9026 train_loss: 0.276738\tval_acc: 0.990196 val_loss: 0.0648356\n",
      "5_391 train_acc: 0.8689 train_loss: 0.337776\tval_acc: 0.980392 val_loss: 0.0645804\n",
      "5_411 train_acc: 0.8614 train_loss: 0.355502\tval_acc: 0.995098 val_loss: 0.0614749\n",
      "5_419 train_acc: 0.8801 train_loss: 0.329375\tval_acc: 0.990196 val_loss: 0.0593953\n",
      "5_469 train_acc: 0.8727 train_loss: 0.336148\tval_acc: 0.980392 val_loss: 0.0577769\n",
      "5_489 train_acc: 0.8876 train_loss: 0.304287\tval_acc: 0.980392 val_loss: 0.0552239\n",
      "5_501 train_acc: 0.9026 train_loss: 0.260429\tval_acc: 0.990196 val_loss: 0.0546806\n",
      "5_522 train_acc: 0.8914 train_loss: 0.316280\tval_acc: 0.990196 val_loss: 0.0539245\n",
      "5_523 train_acc: 0.8839 train_loss: 0.306828\tval_acc: 0.990196 val_loss: 0.0513113\n",
      "5_529 train_acc: 0.8989 train_loss: 0.290507\tval_acc: 0.995098 val_loss: 0.0460064\n",
      "5_554 train_acc: 0.9026 train_loss: 0.283711\tval_acc: 0.990196 val_loss: 0.0445445\n",
      "5_571 train_acc: 0.9026 train_loss: 0.288650\tval_acc: 0.995098 val_loss: 0.0426113\n",
      "5_590 train_acc: 0.8876 train_loss: 0.298781\tval_acc: 1.000000 val_loss: 0.0371944\n",
      "5_605 train_acc: 0.8652 train_loss: 0.325555\tval_acc: 1.000000 val_loss: 0.0371783\n",
      "5_639 train_acc: 0.8764 train_loss: 0.328685\tval_acc: 0.995098 val_loss: 0.0365436\n",
      "5_647 train_acc: 0.8876 train_loss: 0.285811\tval_acc: 0.995098 val_loss: 0.0352575\n",
      "5_696 train_acc: 0.9026 train_loss: 0.245097\tval_acc: 1.000000 val_loss: 0.0340277\n",
      "5_697 train_acc: 0.8764 train_loss: 0.300310\tval_acc: 0.995098 val_loss: 0.0324448\n",
      "5_728 train_acc: 0.8989 train_loss: 0.351715\tval_acc: 0.995098 val_loss: 0.0318112\n",
      "5_751 train_acc: 0.8989 train_loss: 0.258947\tval_acc: 1.000000 val_loss: 0.0260981\n",
      "5_817 train_acc: 0.9101 train_loss: 0.243295\tval_acc: 0.995098 val_loss: 0.0255479\n",
      "5_854 train_acc: 0.9139 train_loss: 0.181197\tval_acc: 1.000000 val_loss: 0.0226847\n",
      "5_879 train_acc: 0.9288 train_loss: 0.207087\tval_acc: 1.000000 val_loss: 0.0221022\n",
      "5_965 train_acc: 0.9139 train_loss: 0.240753\tval_acc: 1.000000 val_loss: 0.0220142\n",
      "5_981 train_acc: 0.9213 train_loss: 0.214380\tval_acc: 1.000000 val_loss: 0.0164184\n",
      "epoch:  981 \tThe test accuracy is: 0.7604166666666666\n",
      " THE BEST ACCURACY IS 0.7604166666666666\tkappa is 0.6805555555555556\n",
      "subject 5 duration: 0:20:06.061406\n",
      "seed is 1677\n",
      "Subject 6\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "6_0 train_acc: 0.2697 train_loss: 2.443885\tval_acc: 0.250000 val_loss: 1.4697417\n",
      "6_2 train_acc: 0.2434 train_loss: 2.250714\tval_acc: 0.303922 val_loss: 1.3911682\n",
      "6_3 train_acc: 0.3333 train_loss: 1.836628\tval_acc: 0.348039 val_loss: 1.3268436\n",
      "6_4 train_acc: 0.3071 train_loss: 1.899857\tval_acc: 0.406863 val_loss: 1.3038640\n",
      "6_5 train_acc: 0.3184 train_loss: 1.699381\tval_acc: 0.436275 val_loss: 1.2440102\n",
      "6_6 train_acc: 0.3408 train_loss: 1.672594\tval_acc: 0.490196 val_loss: 1.1927440\n",
      "6_7 train_acc: 0.3333 train_loss: 1.582078\tval_acc: 0.519608 val_loss: 1.1405501\n",
      "6_8 train_acc: 0.3221 train_loss: 1.576223\tval_acc: 0.549020 val_loss: 1.1294812\n",
      "6_9 train_acc: 0.3783 train_loss: 1.459518\tval_acc: 0.549020 val_loss: 1.1108198\n",
      "6_10 train_acc: 0.4007 train_loss: 1.407843\tval_acc: 0.573529 val_loss: 1.0662770\n",
      "6_11 train_acc: 0.4082 train_loss: 1.413992\tval_acc: 0.607843 val_loss: 1.0413170\n",
      "6_12 train_acc: 0.4120 train_loss: 1.330086\tval_acc: 0.607843 val_loss: 1.0260007\n",
      "6_13 train_acc: 0.4307 train_loss: 1.312994\tval_acc: 0.715686 val_loss: 0.9507286\n",
      "6_15 train_acc: 0.4569 train_loss: 1.228302\tval_acc: 0.666667 val_loss: 0.9333615\n",
      "6_16 train_acc: 0.5056 train_loss: 1.190570\tval_acc: 0.686275 val_loss: 0.9108846\n",
      "6_18 train_acc: 0.5206 train_loss: 1.154384\tval_acc: 0.705882 val_loss: 0.8550015\n",
      "6_19 train_acc: 0.5618 train_loss: 1.080216\tval_acc: 0.700980 val_loss: 0.8362116\n",
      "6_22 train_acc: 0.5918 train_loss: 1.018405\tval_acc: 0.720588 val_loss: 0.8015624\n",
      "6_24 train_acc: 0.5281 train_loss: 1.087539\tval_acc: 0.700980 val_loss: 0.7985893\n",
      "6_25 train_acc: 0.4981 train_loss: 1.068819\tval_acc: 0.691176 val_loss: 0.7867898\n",
      "6_26 train_acc: 0.5243 train_loss: 1.043582\tval_acc: 0.754902 val_loss: 0.7225021\n",
      "6_29 train_acc: 0.6330 train_loss: 0.949591\tval_acc: 0.745098 val_loss: 0.7001488\n",
      "6_30 train_acc: 0.6554 train_loss: 0.836500\tval_acc: 0.769608 val_loss: 0.6820667\n",
      "6_31 train_acc: 0.6779 train_loss: 0.885941\tval_acc: 0.754902 val_loss: 0.6599306\n",
      "6_32 train_acc: 0.6292 train_loss: 0.933741\tval_acc: 0.779412 val_loss: 0.6530225\n",
      "6_33 train_acc: 0.6217 train_loss: 0.936224\tval_acc: 0.774510 val_loss: 0.6075507\n",
      "6_37 train_acc: 0.6517 train_loss: 0.893888\tval_acc: 0.813725 val_loss: 0.5928963\n",
      "6_38 train_acc: 0.6442 train_loss: 0.808770\tval_acc: 0.808824 val_loss: 0.5901707\n",
      "6_39 train_acc: 0.6142 train_loss: 0.895375\tval_acc: 0.813725 val_loss: 0.5662033\n",
      "6_42 train_acc: 0.6629 train_loss: 0.854540\tval_acc: 0.818627 val_loss: 0.5350001\n",
      "6_44 train_acc: 0.7191 train_loss: 0.759157\tval_acc: 0.833333 val_loss: 0.5114436\n",
      "6_49 train_acc: 0.7079 train_loss: 0.736389\tval_acc: 0.818627 val_loss: 0.5053837\n",
      "6_54 train_acc: 0.6891 train_loss: 0.741450\tval_acc: 0.823529 val_loss: 0.4918247\n",
      "6_55 train_acc: 0.7041 train_loss: 0.770219\tval_acc: 0.852941 val_loss: 0.4763049\n",
      "6_60 train_acc: 0.7041 train_loss: 0.731197\tval_acc: 0.867647 val_loss: 0.4646879\n",
      "6_62 train_acc: 0.7154 train_loss: 0.732529\tval_acc: 0.848039 val_loss: 0.4588151\n",
      "6_63 train_acc: 0.7266 train_loss: 0.686423\tval_acc: 0.862745 val_loss: 0.4461208\n",
      "6_67 train_acc: 0.7191 train_loss: 0.744605\tval_acc: 0.852941 val_loss: 0.4284016\n",
      "6_70 train_acc: 0.7154 train_loss: 0.771079\tval_acc: 0.897059 val_loss: 0.4057859\n",
      "6_80 train_acc: 0.7228 train_loss: 0.714435\tval_acc: 0.843137 val_loss: 0.4047812\n",
      "6_83 train_acc: 0.7228 train_loss: 0.645496\tval_acc: 0.857843 val_loss: 0.3892116\n",
      "6_87 train_acc: 0.7154 train_loss: 0.717582\tval_acc: 0.852941 val_loss: 0.3807979\n",
      "6_89 train_acc: 0.7416 train_loss: 0.678196\tval_acc: 0.862745 val_loss: 0.3796731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_90 train_acc: 0.7715 train_loss: 0.542261\tval_acc: 0.867647 val_loss: 0.3709896\n",
      "6_91 train_acc: 0.7266 train_loss: 0.762061\tval_acc: 0.872549 val_loss: 0.3654485\n",
      "6_96 train_acc: 0.7678 train_loss: 0.566222\tval_acc: 0.862745 val_loss: 0.3533607\n",
      "6_102 train_acc: 0.7154 train_loss: 0.701094\tval_acc: 0.877451 val_loss: 0.3504062\n",
      "6_104 train_acc: 0.7715 train_loss: 0.571141\tval_acc: 0.887255 val_loss: 0.3344275\n",
      "6_111 train_acc: 0.7566 train_loss: 0.558341\tval_acc: 0.897059 val_loss: 0.3094836\n",
      "6_127 train_acc: 0.7341 train_loss: 0.547154\tval_acc: 0.906863 val_loss: 0.2992566\n",
      "6_136 train_acc: 0.8202 train_loss: 0.501302\tval_acc: 0.887255 val_loss: 0.2866499\n",
      "6_149 train_acc: 0.8015 train_loss: 0.554018\tval_acc: 0.892157 val_loss: 0.2865650\n",
      "6_153 train_acc: 0.8127 train_loss: 0.533188\tval_acc: 0.911765 val_loss: 0.2700042\n",
      "6_168 train_acc: 0.8052 train_loss: 0.506948\tval_acc: 0.897059 val_loss: 0.2648866\n",
      "6_177 train_acc: 0.8015 train_loss: 0.505126\tval_acc: 0.936275 val_loss: 0.2374460\n",
      "6_197 train_acc: 0.7903 train_loss: 0.545310\tval_acc: 0.946078 val_loss: 0.2202735\n",
      "6_208 train_acc: 0.8352 train_loss: 0.484262\tval_acc: 0.955882 val_loss: 0.2145702\n",
      "6_252 train_acc: 0.7715 train_loss: 0.584234\tval_acc: 0.936275 val_loss: 0.1983130\n",
      "6_260 train_acc: 0.8015 train_loss: 0.510196\tval_acc: 0.950980 val_loss: 0.1743304\n",
      "6_289 train_acc: 0.8240 train_loss: 0.447117\tval_acc: 0.960784 val_loss: 0.1703110\n",
      "6_319 train_acc: 0.8352 train_loss: 0.457202\tval_acc: 0.970588 val_loss: 0.1495405\n",
      "6_352 train_acc: 0.8390 train_loss: 0.455843\tval_acc: 0.975490 val_loss: 0.1423329\n",
      "6_359 train_acc: 0.8090 train_loss: 0.473174\tval_acc: 0.970588 val_loss: 0.1343950\n",
      "6_376 train_acc: 0.8764 train_loss: 0.369030\tval_acc: 0.965686 val_loss: 0.1312728\n",
      "6_432 train_acc: 0.8652 train_loss: 0.378487\tval_acc: 0.965686 val_loss: 0.1258741\n",
      "6_467 train_acc: 0.8240 train_loss: 0.460159\tval_acc: 0.960784 val_loss: 0.1184626\n",
      "6_478 train_acc: 0.8352 train_loss: 0.409521\tval_acc: 0.960784 val_loss: 0.1162406\n",
      "6_490 train_acc: 0.8315 train_loss: 0.426340\tval_acc: 0.985294 val_loss: 0.1029657\n",
      "6_550 train_acc: 0.8614 train_loss: 0.352007\tval_acc: 0.980392 val_loss: 0.1014214\n",
      "6_597 train_acc: 0.8951 train_loss: 0.326224\tval_acc: 0.980392 val_loss: 0.0992546\n",
      "6_612 train_acc: 0.8727 train_loss: 0.346535\tval_acc: 0.970588 val_loss: 0.0985311\n",
      "6_630 train_acc: 0.8652 train_loss: 0.365439\tval_acc: 0.985294 val_loss: 0.0842906\n",
      "6_649 train_acc: 0.8989 train_loss: 0.320533\tval_acc: 0.990196 val_loss: 0.0811101\n",
      "6_682 train_acc: 0.9213 train_loss: 0.235834\tval_acc: 0.975490 val_loss: 0.0796656\n",
      "6_694 train_acc: 0.8652 train_loss: 0.344302\tval_acc: 0.980392 val_loss: 0.0753493\n",
      "6_741 train_acc: 0.8727 train_loss: 0.280337\tval_acc: 0.990196 val_loss: 0.0695815\n",
      "6_757 train_acc: 0.8727 train_loss: 0.309326\tval_acc: 0.980392 val_loss: 0.0685520\n",
      "6_792 train_acc: 0.8801 train_loss: 0.302031\tval_acc: 0.995098 val_loss: 0.0609949\n",
      "6_827 train_acc: 0.8801 train_loss: 0.318440\tval_acc: 0.985294 val_loss: 0.0595839\n",
      "6_879 train_acc: 0.8614 train_loss: 0.359327\tval_acc: 0.990196 val_loss: 0.0590348\n",
      "6_893 train_acc: 0.8951 train_loss: 0.315679\tval_acc: 0.980392 val_loss: 0.0568216\n",
      "6_959 train_acc: 0.9026 train_loss: 0.326630\tval_acc: 0.980392 val_loss: 0.0544277\n",
      "6_982 train_acc: 0.9064 train_loss: 0.275009\tval_acc: 0.980392 val_loss: 0.0540315\n",
      "6_992 train_acc: 0.9064 train_loss: 0.304012\tval_acc: 0.990196 val_loss: 0.0530895\n",
      "6_999 train_acc: 0.8502 train_loss: 0.412607\tval_acc: 0.995098 val_loss: 0.0484834\n",
      "epoch:  999 \tThe test accuracy is: 0.6493055555555556\n",
      " THE BEST ACCURACY IS 0.6493055555555556\tkappa is 0.5324074074074074\n",
      "subject 6 duration: 0:20:13.303390\n",
      "seed is 928\n",
      "Subject 7\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "7_0 train_acc: 0.2846 train_loss: 2.569710\tval_acc: 0.269608 val_loss: 1.4252535\n",
      "7_1 train_acc: 0.3296 train_loss: 2.097969\tval_acc: 0.279412 val_loss: 1.3731811\n",
      "7_2 train_acc: 0.3296 train_loss: 2.068525\tval_acc: 0.367647 val_loss: 1.2795455\n",
      "7_3 train_acc: 0.3146 train_loss: 1.936962\tval_acc: 0.450980 val_loss: 1.2170022\n",
      "7_4 train_acc: 0.3708 train_loss: 1.814745\tval_acc: 0.598039 val_loss: 1.1293092\n",
      "7_5 train_acc: 0.4082 train_loss: 1.492784\tval_acc: 0.622549 val_loss: 1.0781771\n",
      "7_6 train_acc: 0.3933 train_loss: 1.500022\tval_acc: 0.700980 val_loss: 0.9626778\n",
      "7_7 train_acc: 0.4232 train_loss: 1.456690\tval_acc: 0.700980 val_loss: 0.8877655\n",
      "7_8 train_acc: 0.4869 train_loss: 1.310148\tval_acc: 0.735294 val_loss: 0.8350443\n",
      "7_9 train_acc: 0.4794 train_loss: 1.250960\tval_acc: 0.759804 val_loss: 0.8098846\n",
      "7_10 train_acc: 0.5206 train_loss: 1.173113\tval_acc: 0.799020 val_loss: 0.7094265\n",
      "7_11 train_acc: 0.5356 train_loss: 1.111894\tval_acc: 0.789216 val_loss: 0.6944997\n",
      "7_12 train_acc: 0.5543 train_loss: 1.098463\tval_acc: 0.818627 val_loss: 0.6454080\n",
      "7_13 train_acc: 0.6142 train_loss: 0.987149\tval_acc: 0.818627 val_loss: 0.6290473\n",
      "7_15 train_acc: 0.6292 train_loss: 0.937668\tval_acc: 0.789216 val_loss: 0.5900320\n",
      "7_16 train_acc: 0.5880 train_loss: 0.973975\tval_acc: 0.794118 val_loss: 0.5581519\n",
      "7_17 train_acc: 0.5655 train_loss: 1.025316\tval_acc: 0.813725 val_loss: 0.5542065\n",
      "7_19 train_acc: 0.6929 train_loss: 0.767742\tval_acc: 0.828431 val_loss: 0.5089046\n",
      "7_20 train_acc: 0.7378 train_loss: 0.734144\tval_acc: 0.828431 val_loss: 0.4907704\n",
      "7_24 train_acc: 0.6629 train_loss: 0.775350\tval_acc: 0.862745 val_loss: 0.4635513\n",
      "7_27 train_acc: 0.7341 train_loss: 0.755884\tval_acc: 0.843137 val_loss: 0.4609038\n",
      "7_28 train_acc: 0.7603 train_loss: 0.658941\tval_acc: 0.867647 val_loss: 0.4256819\n",
      "7_30 train_acc: 0.7453 train_loss: 0.640275\tval_acc: 0.862745 val_loss: 0.4105556\n",
      "7_32 train_acc: 0.7678 train_loss: 0.574675\tval_acc: 0.857843 val_loss: 0.4086563\n",
      "7_35 train_acc: 0.7303 train_loss: 0.680674\tval_acc: 0.882353 val_loss: 0.3974471\n",
      "7_38 train_acc: 0.7453 train_loss: 0.630919\tval_acc: 0.852941 val_loss: 0.3890408\n",
      "7_42 train_acc: 0.7678 train_loss: 0.568793\tval_acc: 0.887255 val_loss: 0.3562501\n",
      "7_46 train_acc: 0.7753 train_loss: 0.591743\tval_acc: 0.897059 val_loss: 0.3321323\n",
      "7_55 train_acc: 0.7865 train_loss: 0.563395\tval_acc: 0.901961 val_loss: 0.2988332\n",
      "7_74 train_acc: 0.7715 train_loss: 0.525850\tval_acc: 0.892157 val_loss: 0.2784567\n",
      "7_78 train_acc: 0.8052 train_loss: 0.576826\tval_acc: 0.887255 val_loss: 0.2597193\n",
      "7_95 train_acc: 0.8352 train_loss: 0.420063\tval_acc: 0.911765 val_loss: 0.2572154\n",
      "7_98 train_acc: 0.8127 train_loss: 0.519875\tval_acc: 0.916667 val_loss: 0.2396069\n",
      "7_115 train_acc: 0.7678 train_loss: 0.505711\tval_acc: 0.926471 val_loss: 0.2285412\n",
      "7_123 train_acc: 0.8240 train_loss: 0.440264\tval_acc: 0.931373 val_loss: 0.2050579\n",
      "7_148 train_acc: 0.8315 train_loss: 0.439453\tval_acc: 0.926471 val_loss: 0.1915733\n",
      "7_155 train_acc: 0.8464 train_loss: 0.407755\tval_acc: 0.946078 val_loss: 0.1759953\n",
      "7_178 train_acc: 0.8614 train_loss: 0.373955\tval_acc: 0.941176 val_loss: 0.1727150\n",
      "7_190 train_acc: 0.8689 train_loss: 0.381858\tval_acc: 0.950980 val_loss: 0.1582679\n",
      "7_199 train_acc: 0.8539 train_loss: 0.433520\tval_acc: 0.941176 val_loss: 0.1540612\n",
      "7_206 train_acc: 0.8240 train_loss: 0.435216\tval_acc: 0.955882 val_loss: 0.1371059\n",
      "7_216 train_acc: 0.8839 train_loss: 0.288903\tval_acc: 0.955882 val_loss: 0.1275661\n",
      "7_224 train_acc: 0.8764 train_loss: 0.292125\tval_acc: 0.955882 val_loss: 0.1136457\n",
      "7_236 train_acc: 0.8614 train_loss: 0.391249\tval_acc: 0.965686 val_loss: 0.0960811\n",
      "7_247 train_acc: 0.8914 train_loss: 0.310524\tval_acc: 0.950980 val_loss: 0.0914916\n",
      "7_254 train_acc: 0.9176 train_loss: 0.222926\tval_acc: 0.965686 val_loss: 0.0893724\n",
      "7_262 train_acc: 0.9401 train_loss: 0.183496\tval_acc: 0.970588 val_loss: 0.0822143\n",
      "7_265 train_acc: 0.9363 train_loss: 0.223614\tval_acc: 0.980392 val_loss: 0.0759753\n",
      "7_268 train_acc: 0.9438 train_loss: 0.183144\tval_acc: 0.980392 val_loss: 0.0703060\n",
      "7_269 train_acc: 0.9064 train_loss: 0.228471\tval_acc: 0.980392 val_loss: 0.0598903\n",
      "7_280 train_acc: 0.9513 train_loss: 0.150148\tval_acc: 0.985294 val_loss: 0.0566142\n",
      "7_292 train_acc: 0.9288 train_loss: 0.193338\tval_acc: 0.980392 val_loss: 0.0482198\n",
      "7_296 train_acc: 0.9288 train_loss: 0.188684\tval_acc: 0.980392 val_loss: 0.0479866\n",
      "7_303 train_acc: 0.9251 train_loss: 0.225491\tval_acc: 0.985294 val_loss: 0.0423561\n",
      "7_313 train_acc: 0.9663 train_loss: 0.112184\tval_acc: 0.985294 val_loss: 0.0410431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7_323 train_acc: 0.9326 train_loss: 0.201233\tval_acc: 0.985294 val_loss: 0.0402221\n",
      "7_324 train_acc: 0.9363 train_loss: 0.174498\tval_acc: 0.985294 val_loss: 0.0384179\n",
      "7_337 train_acc: 0.9326 train_loss: 0.173008\tval_acc: 0.995098 val_loss: 0.0213172\n",
      "7_413 train_acc: 0.9401 train_loss: 0.195256\tval_acc: 0.995098 val_loss: 0.0126396\n",
      "7_468 train_acc: 0.9513 train_loss: 0.140612\tval_acc: 1.000000 val_loss: 0.0119383\n",
      "7_559 train_acc: 0.9700 train_loss: 0.062205\tval_acc: 0.995098 val_loss: 0.0108549\n",
      "7_566 train_acc: 0.9775 train_loss: 0.063677\tval_acc: 1.000000 val_loss: 0.0094584\n",
      "7_581 train_acc: 0.9738 train_loss: 0.061441\tval_acc: 1.000000 val_loss: 0.0090113\n",
      "7_599 train_acc: 0.9588 train_loss: 0.129488\tval_acc: 1.000000 val_loss: 0.0081659\n",
      "7_612 train_acc: 0.9850 train_loss: 0.044685\tval_acc: 1.000000 val_loss: 0.0080768\n",
      "7_632 train_acc: 0.9663 train_loss: 0.064959\tval_acc: 1.000000 val_loss: 0.0080399\n",
      "7_653 train_acc: 0.9625 train_loss: 0.116922\tval_acc: 1.000000 val_loss: 0.0050517\n",
      "7_700 train_acc: 0.9738 train_loss: 0.090363\tval_acc: 1.000000 val_loss: 0.0044043\n",
      "7_735 train_acc: 0.9775 train_loss: 0.065921\tval_acc: 1.000000 val_loss: 0.0041700\n",
      "7_766 train_acc: 0.9588 train_loss: 0.094564\tval_acc: 1.000000 val_loss: 0.0041229\n",
      "7_799 train_acc: 0.9738 train_loss: 0.080308\tval_acc: 1.000000 val_loss: 0.0023561\n",
      "7_885 train_acc: 0.9888 train_loss: 0.065966\tval_acc: 1.000000 val_loss: 0.0012852\n",
      "epoch:  885 \tThe test accuracy is: 0.9027777777777778\n",
      " THE BEST ACCURACY IS 0.9027777777777778\tkappa is 0.8703703703703703\n",
      "subject 7 duration: 0:19:57.543111\n",
      "seed is 919\n",
      "Subject 8\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "8_0 train_acc: 0.2772 train_loss: 2.450103\tval_acc: 0.269608 val_loss: 1.4124147\n",
      "8_1 train_acc: 0.3184 train_loss: 2.043361\tval_acc: 0.397059 val_loss: 1.2897199\n",
      "8_2 train_acc: 0.2884 train_loss: 2.158177\tval_acc: 0.416667 val_loss: 1.2647251\n",
      "8_3 train_acc: 0.3034 train_loss: 1.754096\tval_acc: 0.416667 val_loss: 1.2514106\n",
      "8_4 train_acc: 0.3333 train_loss: 1.601584\tval_acc: 0.529412 val_loss: 1.1564740\n",
      "8_5 train_acc: 0.3895 train_loss: 1.505195\tval_acc: 0.573529 val_loss: 1.0696362\n",
      "8_6 train_acc: 0.3670 train_loss: 1.622988\tval_acc: 0.568627 val_loss: 1.0433848\n",
      "8_7 train_acc: 0.3820 train_loss: 1.462869\tval_acc: 0.671569 val_loss: 0.9490379\n",
      "8_8 train_acc: 0.4494 train_loss: 1.360772\tval_acc: 0.651961 val_loss: 0.9096034\n",
      "8_9 train_acc: 0.4457 train_loss: 1.313538\tval_acc: 0.696078 val_loss: 0.8866757\n",
      "8_10 train_acc: 0.4944 train_loss: 1.230083\tval_acc: 0.720588 val_loss: 0.8454789\n",
      "8_11 train_acc: 0.4794 train_loss: 1.170059\tval_acc: 0.691176 val_loss: 0.8182628\n",
      "8_12 train_acc: 0.4944 train_loss: 1.215699\tval_acc: 0.720588 val_loss: 0.7595938\n",
      "8_14 train_acc: 0.5318 train_loss: 1.032898\tval_acc: 0.764706 val_loss: 0.7067780\n",
      "8_15 train_acc: 0.6180 train_loss: 0.963250\tval_acc: 0.750000 val_loss: 0.7044313\n",
      "8_17 train_acc: 0.5581 train_loss: 1.010636\tval_acc: 0.803922 val_loss: 0.6711702\n",
      "8_19 train_acc: 0.6629 train_loss: 0.873239\tval_acc: 0.784314 val_loss: 0.6406706\n",
      "8_20 train_acc: 0.6404 train_loss: 0.821906\tval_acc: 0.808824 val_loss: 0.5979068\n",
      "8_21 train_acc: 0.6180 train_loss: 0.916474\tval_acc: 0.789216 val_loss: 0.5716911\n",
      "8_25 train_acc: 0.6629 train_loss: 0.780590\tval_acc: 0.838235 val_loss: 0.5469127\n",
      "8_27 train_acc: 0.7154 train_loss: 0.688939\tval_acc: 0.838235 val_loss: 0.5223360\n",
      "8_28 train_acc: 0.7453 train_loss: 0.681222\tval_acc: 0.833333 val_loss: 0.4950076\n",
      "8_29 train_acc: 0.6891 train_loss: 0.714819\tval_acc: 0.843137 val_loss: 0.4756417\n",
      "8_32 train_acc: 0.6891 train_loss: 0.817849\tval_acc: 0.867647 val_loss: 0.4255089\n",
      "8_33 train_acc: 0.7041 train_loss: 0.744968\tval_acc: 0.892157 val_loss: 0.4108615\n",
      "8_38 train_acc: 0.7116 train_loss: 0.664192\tval_acc: 0.872549 val_loss: 0.4065086\n",
      "8_41 train_acc: 0.7041 train_loss: 0.722507\tval_acc: 0.887255 val_loss: 0.3861560\n",
      "8_42 train_acc: 0.7640 train_loss: 0.581245\tval_acc: 0.887255 val_loss: 0.3799400\n",
      "8_44 train_acc: 0.7790 train_loss: 0.541559\tval_acc: 0.872549 val_loss: 0.3653368\n",
      "8_46 train_acc: 0.7828 train_loss: 0.547419\tval_acc: 0.887255 val_loss: 0.3587490\n",
      "8_47 train_acc: 0.7566 train_loss: 0.625296\tval_acc: 0.897059 val_loss: 0.3523490\n",
      "8_48 train_acc: 0.7940 train_loss: 0.567961\tval_acc: 0.916667 val_loss: 0.3237717\n",
      "8_49 train_acc: 0.7416 train_loss: 0.639466\tval_acc: 0.906863 val_loss: 0.3158612\n",
      "8_50 train_acc: 0.7828 train_loss: 0.580661\tval_acc: 0.921569 val_loss: 0.2998697\n",
      "8_53 train_acc: 0.7865 train_loss: 0.579603\tval_acc: 0.926471 val_loss: 0.2982882\n",
      "8_55 train_acc: 0.7903 train_loss: 0.560807\tval_acc: 0.931373 val_loss: 0.2820536\n",
      "8_60 train_acc: 0.7978 train_loss: 0.527748\tval_acc: 0.901961 val_loss: 0.2801703\n",
      "8_63 train_acc: 0.7790 train_loss: 0.607508\tval_acc: 0.921569 val_loss: 0.2764403\n",
      "8_64 train_acc: 0.8090 train_loss: 0.463676\tval_acc: 0.906863 val_loss: 0.2679560\n",
      "8_70 train_acc: 0.8427 train_loss: 0.407193\tval_acc: 0.916667 val_loss: 0.2364833\n",
      "8_75 train_acc: 0.8052 train_loss: 0.510019\tval_acc: 0.955882 val_loss: 0.2306776\n",
      "8_76 train_acc: 0.8277 train_loss: 0.455173\tval_acc: 0.926471 val_loss: 0.2161181\n",
      "8_79 train_acc: 0.8801 train_loss: 0.372552\tval_acc: 0.936275 val_loss: 0.2011951\n",
      "8_83 train_acc: 0.8427 train_loss: 0.402399\tval_acc: 0.950980 val_loss: 0.1749281\n",
      "8_90 train_acc: 0.8390 train_loss: 0.377386\tval_acc: 0.946078 val_loss: 0.1690878\n",
      "8_95 train_acc: 0.8315 train_loss: 0.378182\tval_acc: 0.941176 val_loss: 0.1636282\n",
      "8_97 train_acc: 0.8502 train_loss: 0.436886\tval_acc: 0.946078 val_loss: 0.1614990\n",
      "8_100 train_acc: 0.8727 train_loss: 0.326585\tval_acc: 0.955882 val_loss: 0.1389639\n",
      "8_106 train_acc: 0.8839 train_loss: 0.316890\tval_acc: 0.960784 val_loss: 0.1246602\n",
      "8_114 train_acc: 0.8390 train_loss: 0.344975\tval_acc: 0.965686 val_loss: 0.1205082\n",
      "8_118 train_acc: 0.8951 train_loss: 0.331100\tval_acc: 0.965686 val_loss: 0.1197321\n",
      "8_126 train_acc: 0.8727 train_loss: 0.323451\tval_acc: 0.965686 val_loss: 0.1146591\n",
      "8_132 train_acc: 0.8727 train_loss: 0.349548\tval_acc: 0.970588 val_loss: 0.1070032\n",
      "8_138 train_acc: 0.9139 train_loss: 0.214815\tval_acc: 0.975490 val_loss: 0.1024023\n",
      "8_139 train_acc: 0.8427 train_loss: 0.421228\tval_acc: 0.980392 val_loss: 0.0994328\n",
      "8_140 train_acc: 0.8914 train_loss: 0.252223\tval_acc: 0.980392 val_loss: 0.0865896\n",
      "8_143 train_acc: 0.8764 train_loss: 0.271872\tval_acc: 0.980392 val_loss: 0.0810681\n",
      "8_155 train_acc: 0.8914 train_loss: 0.289036\tval_acc: 0.990196 val_loss: 0.0790168\n",
      "8_157 train_acc: 0.8539 train_loss: 0.350804\tval_acc: 0.985294 val_loss: 0.0754900\n",
      "8_166 train_acc: 0.9064 train_loss: 0.240776\tval_acc: 0.980392 val_loss: 0.0754177\n",
      "8_173 train_acc: 0.9176 train_loss: 0.236686\tval_acc: 0.980392 val_loss: 0.0653717\n",
      "8_193 train_acc: 0.8727 train_loss: 0.345925\tval_acc: 0.975490 val_loss: 0.0598565\n",
      "8_205 train_acc: 0.9288 train_loss: 0.231627\tval_acc: 0.980392 val_loss: 0.0598147\n",
      "8_227 train_acc: 0.9213 train_loss: 0.233898\tval_acc: 0.985294 val_loss: 0.0467556\n",
      "8_260 train_acc: 0.9401 train_loss: 0.204222\tval_acc: 0.995098 val_loss: 0.0380008\n",
      "8_297 train_acc: 0.9476 train_loss: 0.130679\tval_acc: 0.985294 val_loss: 0.0342767\n",
      "8_319 train_acc: 0.9363 train_loss: 0.186573\tval_acc: 0.990196 val_loss: 0.0287115\n",
      "8_321 train_acc: 0.9251 train_loss: 0.193768\tval_acc: 0.995098 val_loss: 0.0278520\n",
      "8_336 train_acc: 0.9288 train_loss: 0.176011\tval_acc: 1.000000 val_loss: 0.0230161\n",
      "8_360 train_acc: 0.9326 train_loss: 0.203665\tval_acc: 0.995098 val_loss: 0.0222493\n",
      "8_387 train_acc: 0.9438 train_loss: 0.151475\tval_acc: 0.995098 val_loss: 0.0216378\n",
      "8_399 train_acc: 0.9438 train_loss: 0.120187\tval_acc: 1.000000 val_loss: 0.0173566\n",
      "8_452 train_acc: 0.9326 train_loss: 0.197968\tval_acc: 1.000000 val_loss: 0.0147091\n",
      "8_473 train_acc: 0.9513 train_loss: 0.131224\tval_acc: 1.000000 val_loss: 0.0145704\n",
      "8_481 train_acc: 0.9401 train_loss: 0.172949\tval_acc: 1.000000 val_loss: 0.0136211\n",
      "8_485 train_acc: 0.9476 train_loss: 0.160881\tval_acc: 1.000000 val_loss: 0.0118442\n",
      "8_488 train_acc: 0.9438 train_loss: 0.164764\tval_acc: 1.000000 val_loss: 0.0090458\n",
      "8_522 train_acc: 0.9663 train_loss: 0.118072\tval_acc: 1.000000 val_loss: 0.0076827\n",
      "8_627 train_acc: 0.9363 train_loss: 0.192827\tval_acc: 1.000000 val_loss: 0.0061690\n",
      "8_675 train_acc: 0.9700 train_loss: 0.117602\tval_acc: 1.000000 val_loss: 0.0057787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_714 train_acc: 0.9438 train_loss: 0.140781\tval_acc: 1.000000 val_loss: 0.0046548\n",
      "8_720 train_acc: 0.9513 train_loss: 0.115896\tval_acc: 1.000000 val_loss: 0.0043697\n",
      "8_791 train_acc: 0.9513 train_loss: 0.131804\tval_acc: 1.000000 val_loss: 0.0032694\n",
      "8_845 train_acc: 0.9738 train_loss: 0.089156\tval_acc: 1.000000 val_loss: 0.0030615\n",
      "8_871 train_acc: 0.9551 train_loss: 0.127484\tval_acc: 1.000000 val_loss: 0.0028688\n",
      "8_904 train_acc: 0.9738 train_loss: 0.100220\tval_acc: 1.000000 val_loss: 0.0028444\n",
      "8_906 train_acc: 0.9401 train_loss: 0.115065\tval_acc: 1.000000 val_loss: 0.0025835\n",
      "8_994 train_acc: 0.9700 train_loss: 0.084614\tval_acc: 1.000000 val_loss: 0.0021090\n",
      "epoch:  994 \tThe test accuracy is: 0.875\n",
      " THE BEST ACCURACY IS 0.875\tkappa is 0.8333333333333334\n",
      "subject 8 duration: 0:20:28.284846\n",
      "seed is 1195\n",
      "Subject 9\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "9_0 train_acc: 0.2434 train_loss: 2.446332\tval_acc: 0.245098 val_loss: 1.4741368\n",
      "9_1 train_acc: 0.2772 train_loss: 2.084550\tval_acc: 0.250000 val_loss: 1.4636029\n",
      "9_2 train_acc: 0.2772 train_loss: 2.070243\tval_acc: 0.299020 val_loss: 1.3744364\n",
      "9_3 train_acc: 0.3558 train_loss: 1.810146\tval_acc: 0.333333 val_loss: 1.3158182\n",
      "9_4 train_acc: 0.3933 train_loss: 1.518455\tval_acc: 0.455882 val_loss: 1.2172371\n",
      "9_5 train_acc: 0.3933 train_loss: 1.493088\tval_acc: 0.539216 val_loss: 1.1337492\n",
      "9_6 train_acc: 0.4307 train_loss: 1.502977\tval_acc: 0.602941 val_loss: 1.0470307\n",
      "9_7 train_acc: 0.4607 train_loss: 1.237168\tval_acc: 0.656863 val_loss: 0.9452608\n",
      "9_8 train_acc: 0.4195 train_loss: 1.337392\tval_acc: 0.651961 val_loss: 0.9332910\n",
      "9_9 train_acc: 0.4345 train_loss: 1.288331\tval_acc: 0.754902 val_loss: 0.8649831\n",
      "9_10 train_acc: 0.4644 train_loss: 1.211068\tval_acc: 0.750000 val_loss: 0.8254716\n",
      "9_11 train_acc: 0.5056 train_loss: 1.158446\tval_acc: 0.740196 val_loss: 0.7913472\n",
      "9_12 train_acc: 0.6255 train_loss: 0.986916\tval_acc: 0.774510 val_loss: 0.7337468\n",
      "9_13 train_acc: 0.4944 train_loss: 1.171409\tval_acc: 0.779412 val_loss: 0.7202699\n",
      "9_14 train_acc: 0.5768 train_loss: 1.030782\tval_acc: 0.808824 val_loss: 0.6406165\n",
      "9_18 train_acc: 0.6292 train_loss: 0.912837\tval_acc: 0.779412 val_loss: 0.6169474\n",
      "9_19 train_acc: 0.6217 train_loss: 0.884934\tval_acc: 0.813725 val_loss: 0.5661986\n",
      "9_22 train_acc: 0.6554 train_loss: 0.840228\tval_acc: 0.808824 val_loss: 0.5609766\n",
      "9_24 train_acc: 0.6554 train_loss: 0.903099\tval_acc: 0.799020 val_loss: 0.5449401\n",
      "9_25 train_acc: 0.6554 train_loss: 0.876658\tval_acc: 0.823529 val_loss: 0.5198539\n",
      "9_28 train_acc: 0.6592 train_loss: 0.809897\tval_acc: 0.803922 val_loss: 0.5046300\n",
      "9_29 train_acc: 0.6629 train_loss: 0.790065\tval_acc: 0.823529 val_loss: 0.5038214\n",
      "9_30 train_acc: 0.7079 train_loss: 0.738679\tval_acc: 0.852941 val_loss: 0.4519409\n",
      "9_37 train_acc: 0.7416 train_loss: 0.622802\tval_acc: 0.843137 val_loss: 0.4500901\n",
      "9_39 train_acc: 0.6742 train_loss: 0.743064\tval_acc: 0.843137 val_loss: 0.4438421\n",
      "9_40 train_acc: 0.7790 train_loss: 0.542675\tval_acc: 0.882353 val_loss: 0.4186172\n",
      "9_43 train_acc: 0.7978 train_loss: 0.584830\tval_acc: 0.852941 val_loss: 0.4147446\n",
      "9_44 train_acc: 0.7528 train_loss: 0.604075\tval_acc: 0.872549 val_loss: 0.4044057\n",
      "9_45 train_acc: 0.7715 train_loss: 0.602368\tval_acc: 0.887255 val_loss: 0.3853029\n",
      "9_46 train_acc: 0.7378 train_loss: 0.594464\tval_acc: 0.882353 val_loss: 0.3610853\n",
      "9_50 train_acc: 0.7790 train_loss: 0.562971\tval_acc: 0.887255 val_loss: 0.3416672\n",
      "9_59 train_acc: 0.7828 train_loss: 0.573639\tval_acc: 0.906863 val_loss: 0.3320462\n",
      "9_61 train_acc: 0.7790 train_loss: 0.591374\tval_acc: 0.916667 val_loss: 0.2986686\n",
      "9_62 train_acc: 0.7491 train_loss: 0.609778\tval_acc: 0.916667 val_loss: 0.2910360\n",
      "9_67 train_acc: 0.8165 train_loss: 0.508902\tval_acc: 0.916667 val_loss: 0.2818378\n",
      "9_75 train_acc: 0.8165 train_loss: 0.476304\tval_acc: 0.911765 val_loss: 0.2765631\n",
      "9_77 train_acc: 0.8127 train_loss: 0.469801\tval_acc: 0.906863 val_loss: 0.2531474\n",
      "9_83 train_acc: 0.8315 train_loss: 0.459086\tval_acc: 0.921569 val_loss: 0.2293647\n",
      "9_91 train_acc: 0.8464 train_loss: 0.432629\tval_acc: 0.931373 val_loss: 0.2215533\n",
      "9_93 train_acc: 0.7865 train_loss: 0.521995\tval_acc: 0.926471 val_loss: 0.2019062\n",
      "9_103 train_acc: 0.8652 train_loss: 0.324815\tval_acc: 0.936275 val_loss: 0.1895294\n",
      "9_108 train_acc: 0.8464 train_loss: 0.387324\tval_acc: 0.936275 val_loss: 0.1720292\n",
      "9_116 train_acc: 0.8502 train_loss: 0.358614\tval_acc: 0.941176 val_loss: 0.1624862\n",
      "9_131 train_acc: 0.8464 train_loss: 0.396719\tval_acc: 0.936275 val_loss: 0.1421231\n",
      "9_136 train_acc: 0.8727 train_loss: 0.346993\tval_acc: 0.936275 val_loss: 0.1370250\n",
      "9_145 train_acc: 0.9101 train_loss: 0.281386\tval_acc: 0.950980 val_loss: 0.1322917\n",
      "9_156 train_acc: 0.8427 train_loss: 0.352665\tval_acc: 0.946078 val_loss: 0.1302907\n",
      "9_162 train_acc: 0.9026 train_loss: 0.228673\tval_acc: 0.970588 val_loss: 0.1053574\n",
      "9_163 train_acc: 0.8876 train_loss: 0.248655\tval_acc: 0.975490 val_loss: 0.1042092\n",
      "9_182 train_acc: 0.8914 train_loss: 0.286775\tval_acc: 0.960784 val_loss: 0.1039007\n",
      "9_194 train_acc: 0.9026 train_loss: 0.249055\tval_acc: 0.960784 val_loss: 0.1014682\n",
      "9_197 train_acc: 0.8951 train_loss: 0.246815\tval_acc: 0.970588 val_loss: 0.1006866\n",
      "9_201 train_acc: 0.9139 train_loss: 0.221458\tval_acc: 0.965686 val_loss: 0.0806429\n",
      "9_208 train_acc: 0.9288 train_loss: 0.238600\tval_acc: 0.970588 val_loss: 0.0774134\n",
      "9_214 train_acc: 0.9064 train_loss: 0.264938\tval_acc: 0.985294 val_loss: 0.0697747\n",
      "9_248 train_acc: 0.9101 train_loss: 0.202190\tval_acc: 0.975490 val_loss: 0.0693103\n",
      "9_256 train_acc: 0.9401 train_loss: 0.175298\tval_acc: 0.970588 val_loss: 0.0634620\n",
      "9_261 train_acc: 0.9288 train_loss: 0.192545\tval_acc: 0.990196 val_loss: 0.0506423\n",
      "9_290 train_acc: 0.9026 train_loss: 0.236401\tval_acc: 0.985294 val_loss: 0.0395795\n",
      "9_315 train_acc: 0.9288 train_loss: 0.172902\tval_acc: 1.000000 val_loss: 0.0324287\n",
      "9_369 train_acc: 0.9588 train_loss: 0.142964\tval_acc: 0.990196 val_loss: 0.0297304\n",
      "9_372 train_acc: 0.9476 train_loss: 0.132042\tval_acc: 0.990196 val_loss: 0.0287816\n",
      "9_378 train_acc: 0.9438 train_loss: 0.171469\tval_acc: 0.995098 val_loss: 0.0226244\n",
      "9_401 train_acc: 0.9551 train_loss: 0.105400\tval_acc: 1.000000 val_loss: 0.0183631\n",
      "9_433 train_acc: 0.9625 train_loss: 0.102708\tval_acc: 1.000000 val_loss: 0.0139256\n",
      "9_476 train_acc: 0.9663 train_loss: 0.090133\tval_acc: 1.000000 val_loss: 0.0120583\n",
      "9_516 train_acc: 0.9625 train_loss: 0.096190\tval_acc: 1.000000 val_loss: 0.0109085\n",
      "9_553 train_acc: 0.9700 train_loss: 0.104353\tval_acc: 0.995098 val_loss: 0.0098775\n",
      "9_555 train_acc: 0.9551 train_loss: 0.166691\tval_acc: 1.000000 val_loss: 0.0093387\n",
      "9_565 train_acc: 0.9401 train_loss: 0.140388\tval_acc: 1.000000 val_loss: 0.0086307\n",
      "9_577 train_acc: 0.9738 train_loss: 0.091923\tval_acc: 1.000000 val_loss: 0.0048851\n",
      "9_685 train_acc: 0.9775 train_loss: 0.092966\tval_acc: 1.000000 val_loss: 0.0047069\n",
      "9_702 train_acc: 0.9625 train_loss: 0.099098\tval_acc: 1.000000 val_loss: 0.0038626\n",
      "9_707 train_acc: 0.9700 train_loss: 0.067040\tval_acc: 1.000000 val_loss: 0.0034775\n",
      "9_744 train_acc: 0.9551 train_loss: 0.099511\tval_acc: 1.000000 val_loss: 0.0026484\n",
      "9_814 train_acc: 0.9625 train_loss: 0.104651\tval_acc: 1.000000 val_loss: 0.0016768\n",
      "9_886 train_acc: 0.9588 train_loss: 0.111833\tval_acc: 1.000000 val_loss: 0.0015427\n",
      "9_950 train_acc: 0.9738 train_loss: 0.050145\tval_acc: 1.000000 val_loss: 0.0013524\n",
      "9_978 train_acc: 0.9700 train_loss: 0.082279\tval_acc: 1.000000 val_loss: 0.0011881\n",
      "9_982 train_acc: 0.9738 train_loss: 0.082626\tval_acc: 1.000000 val_loss: 0.0009414\n",
      "epoch:  982 \tThe test accuracy is: 0.8854166666666666\n",
      " THE BEST ACCURACY IS 0.8854166666666666\tkappa is 0.8472222222222222\n",
      "subject 9 duration: 0:20:00.687762\n",
      "**The average Best accuracy is: 82.90895061728395kappa is: 77.2119341563786\n",
      "\n",
      "best epochs:  [979, 974, 981, 978, 981, 999, 885, 994, 982]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     90.972222  91.502371  90.972222  90.852945  87.962963\n",
      "1     72.916667  73.338755  72.916667  72.839024  63.888889\n",
      "2     92.013889  92.472678  92.013889  92.035621  89.351852\n",
      "3     82.986111  84.171815  82.986111  83.015800  77.314815\n",
      "4     76.041667  76.114766  76.041667  75.858290  68.055556\n",
      "5     64.930556  66.089729  64.930556  64.952595  53.240741\n",
      "6     90.277778  91.218017  90.277778  90.129407  87.037037\n",
      "7     87.500000  87.750137  87.500000  87.592122  83.333333\n",
      "8     88.541667  88.717764  88.541667  88.572089  84.722222\n",
      "mean  82.908951  83.486226  82.908951  82.871988  77.211934\n",
      "std    9.521391   9.425783   9.521391   9.526847  12.695188\n",
      "****************************************\n",
      "Thu Oct 17 07:50:49 2024\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CTNet: A Convolution-Transformer Network for EEG-Based Motor Imagery Classification\n",
    "\n",
    "author: zhaowei701@163.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "\n",
    "            # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        \n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                ), emb_size, drop_p)\n",
    "            \n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "#             TransformerEncoder(heads, depth, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "        \n",
    "   \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size)\n",
    "#         self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "        # self.cnn_module = Branchcnn_moduleTransformer(heads, depth, emb_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "\n",
    "        # add label \n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        \n",
    "        trans = self.trans(cnn)\n",
    "        \n",
    "        features = cnn+trans\n",
    "        \n",
    "        \n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            \n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        # print(\"len(self.allData):\", len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        # print(\"shuffle_num\", shuffle_num)\n",
    "        # print(\"self.allLabel\", self.allLabel)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "\n",
    "        print('-'*20, \"train size：\", self.train_data.shape, \"test size：\", self.test_data.shape)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        img, label, test_data, test_label = self.get_source_data()\n",
    "        # print(\"label size:\", label.shape)\n",
    "        # print(\"label size:\", label)\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        \n",
    "\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            # 验证集\n",
    "            val_data_list = []\n",
    "            val_label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                number_validate = int(self.validate_ratio * number_sample)\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img[:-number_validate]\n",
    "                train_label = label[:-number_validate]\n",
    "                \n",
    "                val_data_list.append(img[number_validate:])\n",
    "                val_label_list.append(label[number_validate:])\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_data = torch.cat(val_data_list).cuda()\n",
    "                val_label = torch.cat(val_label_list).cuda()\n",
    "                val_data = val_data.type(self.Tensor)\n",
    "                val_label = val_label.type(self.LongTensor)            \n",
    "                \n",
    "                val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "                self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "                outputs_list = []\n",
    "                with torch.no_grad():\n",
    "                    for i, (img, _) in enumerate(self.val_dataloader):\n",
    "                        # val model\n",
    "                        img = img.type(self.Tensor).cuda()\n",
    "                        _, Cls = self.model(img)\n",
    "                        outputs_list.append(Cls)\n",
    "                        del img, Cls\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                Cls = torch.cat(outputs_list)\n",
    "                \n",
    "                val_loss = self.criterion_cls(Cls, val_label)\n",
    "                val_pred = torch.max(Cls, 1)[1]\n",
    "                val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if min_loss>val_loss:\n",
    "                    min_loss = val_loss\n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "            del label, val_data, val_label\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent', # 评估模式：LOSO（跨个体）或其他（subject-dependent, subject-specific），\n",
    "         heads=8,             # heads of MHA\n",
    "         emb_size=48,         # token embding dim\n",
    "         depth=3,             # Transformer encoder depth\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         eeg1_f1=20,          # features of temporal conv\n",
    "         eeg1_kernel_size=64, # kernel size of temporal conv\n",
    "         eeg1_D=2,            # depth-wise conv \n",
    "         eeg1_pooling_size1=8,# p1\n",
    "         eeg1_pooling_size2=8,# p2\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    \n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "    \n",
    "    for i in range(N_SUBJECT):      \n",
    "        \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio\n",
    "                  )\n",
    "\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "                \n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'../mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    N_SUBJECT = 9       # BCI \n",
    "    N_AUG = 3           # data augmentation times for benerating artificial training data set\n",
    "    N_SEG = 8           # segmentation times for S&R\n",
    "\n",
    "    EPOCHS = 1000\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'B'\n",
    "    validate_ratio = 0.3 # split raw train dataset into real train dataset and validate dataset\n",
    "\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    \n",
    "    parameters_list = ['A']\n",
    "    for TYPE in parameters_list:\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"CTNet_{}_heads_{}_depth_{}_{}\".format(TYPE, HEADS, DEPTH, int(time.time()))\n",
    "    \n",
    "        sModel = EEGTransformer(\n",
    "            heads=HEADS, \n",
    "            emb_size=EMB_DIM,\n",
    "            depth=DEPTH, \n",
    "            database_type=TYPE,\n",
    "            eeg1_f1=EEGNet1_F1, \n",
    "            eeg1_D=EEGNet1_D,\n",
    "            eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "            eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "            eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "            eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "            eeg1_number_channel = number_channel,\n",
    "            flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "            ).cuda()\n",
    "        summary(sModel, (1, number_channel, 1000)) \n",
    "    \n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        \n",
    "        result = main(RESULT_NAME,\n",
    "                        evaluate_mode = EVALUATE_MODE,\n",
    "                        heads=HEADS, \n",
    "                        emb_size=EMB_DIM,\n",
    "                        depth=DEPTH, \n",
    "                        dataset_type=TYPE,\n",
    "                        eeg1_f1 = EEGNet1_F1,\n",
    "                        eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                        eeg1_D = EEGNet1_D,\n",
    "                        eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                        eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                        eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                        flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                        validate_ratio = validate_ratio,\n",
    "                      )\n",
    "        print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07daa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
